{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Sentiment analysis** is an important application of natural language processing, as it makes it possible to predict what a person thinks given the text she has written. \n",
    "\n",
    "For example, let's say you own a company and you would like to monitor the opinion of your customers on twitter. It's fairly easy to detect the tweets in which your company or products is mentioned, and to find out how many times these tweets are liked or retweeted. \n",
    "\n",
    "But tweets and likes do not mean that people like what you're doing! Maybe they're just destroying the reputation of your company online, or they like a funny tweet in which somebody says your products are really bad. \n",
    "\n",
    "That's where sentiment analysis is needed: it will tell you whether the tweet is positive or negative for your company, and how you should interpret all these likes and retweets.  \n",
    "\n",
    "This post is the third part of my tutorial series about natural language processing with the [yelp dataset](https://www.yelp.com/dataset). You will learn how to classify the reviews of the yelp dataset as positive or negative with two different deep neural networks.\n",
    "\n",
    "First, we will try a simple network consisting of an **embedding layer**, a **dense layer**, and a final **sigmoid neuron**. \n",
    "\n",
    "Then, we will see how **convolutional layers** can help us improve performance. \n",
    "\n",
    "If you don't know the terms used above, you can refer to these tutorials: \n",
    "\n",
    "* **embedding** is explained in [word embedding and simple sentiment analysis](https://thedatafrog.com/word-embedding-sentiment-analysis/);\n",
    "* **dense layers** are introduced in [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/);\n",
    "* **the sigmoid neuron** is described in details in [the 1-neuron network: logistic regression](https://thedatafrog.com/logistic-regression/);\n",
    "* **convolutional layers** are described at length in [tuning a deep convolutional network for image recognition, with keras and tensorflow](https://thedatafrog.com/deep-learning-keras/)\n",
    "\n",
    "## How to run this tutorial\n",
    "\n",
    "A GPU will save you a lot of time, as we want to train fairly complex networks on a large number of events here. **how to access dataset on the google colab platform?**\n",
    "\n",
    "To get access to a GPU for your training, you can simply run this tutorial on the Google Colaboratory platform by clicking [this link](https://colab.research.google.com/github/cbernet/maldives/blob/master/yelp/yelp_simplenet.py.ipynb). Make sure to change the runtime to run on a GPU. \n",
    "\n",
    "The other possibility is to use your own machine. Install Anaconda for python 3.X, TensorFlow, and keras on [Windows](https://thedatafrog.com/install-tensorflow-windows/) or [Linux](https://thedatafrog.com/install-tensorflow-ubuntu/). Then: \n",
    "\n",
    "* get [the repository containing this notebook](https://github.com/cbernet/maldives/archive/master.zip), and unpack the archive\n",
    "* start the jupyter notebook\n",
    "* navigate to the `yelp` directory, and open `yelp_simplenet.ipynb` \n",
    "\n",
    "Now let's initialize our tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual stuff: \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# get reproducible results\n",
    "from numpy.random import seed\n",
    "seed(0xdeadbeef)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(0xdeadbeef)\n",
    "\n",
    "# needed to run on a mac: \n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The dataset\n",
    "\n",
    "The reviews of the [yelp dataset](https://www.yelp.com/dataset) come as a very large JSON lines file containing the reviews in plain text, together with the corresponding rating and some more information. The reviews look like this: \n",
    "\n",
    "```\n",
    "{\"review_id\":\"Q1sbwvVQXV2734tPgoKj4Q\",\"user_id\":\"hG7b0MtEbXx5QzbzE6C_VA\",\n",
    "\"business_id\":\"ujmEBvifdJM6h6RLv4wQIg\",\n",
    "\"stars\":1.0,\"useful\":6,\"funny\":1,\"cool\":0,\n",
    "\"text\":\"Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.\",\"date\":\"2013-05-07 04:34:36\"}\n",
    "```\n",
    "\n",
    "This file needs to be preprocessed for machine learning. Indeed, to feed the review text to a neural network, we need to convert it to an array of numbers in some way, a task called encoding.\n",
    "\n",
    "You can follow [this tutorial](https://thedatafrog.com/text-preprocessing-machine-learning-yelp/) to do the preprocessing yourself. \n",
    "\n",
    "Alternatively, you can download the following two files from [my CERN cloud](https://cernbox.cern.ch/index.php/s/veToy2xMVdzMKpD):  \n",
    "\n",
    "* **data.h5** : the dataset as a numpy array in an hdf5 file. It's a bit more than 3 GB, so you need a fast network connection.\n",
    "* **index.pck** : the vocabulary, that will be needed for the embedding and to decode our reviews into text to that we can read them if needed. \n",
    "\n",
    "Put these two files in a directory called `data_dir`. \n",
    "\n",
    "Now let's open our dataset file. This is an hdf5 file, so we use the h5py package to open it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "# change this to point to the directory \n",
    "# where you saved the two files above: \n",
    "datadir = '/data2/cbernet/maldives/yelp_dataset/'\n",
    "datafile = datadir+'data.h5'\n",
    "h5 = h5py.File(datafile)\n",
    "h5.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the dataset already as a numpy array. h5py will load in memory only the the data you need to complete a given operation. For example, here is the shape of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5['reviews']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's check the first line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At preprocessing stage, when I created this array, I decided to reserve the first four slots on each line for: \n",
    "\n",
    "* the number of stars; \n",
    "* the number of \"useful\" votes;\n",
    "* the number of \"funny\" votes;\n",
    "* the number of \"cool\" votes.\n",
    "\n",
    "The reviewer gave 5 stars (the maximum rating) to this company, and his review was considered useful by somebody.  \n",
    "\n",
    "After the first four slots come the codes for the review text. I allocated 250 slots for the reviews. If the review contains more than 250 words, it's truncated. If it contains less that 250 words, as is the case here, the unused slots are filled with zeros. \n",
    "\n",
    "We can decode this review with the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabulary import Vocabulary\n",
    "# load the vocabulary object from index.pck\n",
    "# (the extension is added automatically)\n",
    "vocab = Vocabulary.load(datadir+'/index')\n",
    "# selecting the text of the first review,\n",
    "# excluding the first 4 slots\n",
    "first_review = data[0,4:]\n",
    "# the decoding returns a list of words, \n",
    "# and we join the words with spaces\n",
    "' '.join( vocab.decode(first_review) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the information needed to train our neural networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reviews (the examples)\n",
    "x = data[:, 4:]\n",
    "# the stars, from which we will\n",
    "# obtain the labels (see below)\n",
    "stars = data[:,0]\n",
    "# additional features we might consider:\n",
    "useful = data[:,1]\n",
    "cool = data[:,2]\n",
    "funny = data[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict whether the review text is positive or negative. Therefore, we need to label our examples in two categories: 0 (negative) and 1 (positive). We can use the number of stars to define these categories. For example, we could say that a review with 3 stars or more is positive. \n",
    "\n",
    "First, let's check the distribution of stars: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(stars[:1000], range=(-0.5, 5.5), bins=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the number of stars ranges from 1 to 5, so it's not possible for a reviewer to give no star. \n",
    "\n",
    "Then, we want to split the dataset in two categories that have roughly the same number of examples.\n",
    "\n",
    "If we were to define as positive the examples with 3 stars or more, the positive category would be much larger than the negative one. \n",
    "\n",
    "I prefer to define as positive all reviews with 4 stars or more. Technically, here is how to define the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first fill an array with 0, \n",
    "# with the same shape as stars\n",
    "y = np.zeros_like(stars)\n",
    "# then write 1 if the number of stars is 4 or 5\n",
    "y[stars>3.5] = 1\n",
    "print(y, len(y))\n",
    "print(stars, len(stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we split the dataset into a training and a test sample. At first, we will use 20000 examples for the test sample, and \"only\" 100,000 examples for the training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "n_train = 100000\n",
    "x_test = x[:n_test]\n",
    "y_test = y[:n_test]\n",
    "x_train = x[n_test:n_train+n_test]\n",
    "y_train = y[n_test:n_train+n_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with a simple dense network\n",
    "\n",
    "Our first deep neural network will contain: \n",
    "\n",
    "* an embedding layer \n",
    "  * cf. [word embedding and simple sentiment analysis](https://thedatafrog.com/word-embedding-sentiment-analysis) for more information about embedding\n",
    "* a dense layer, responsible for interpreting the results of the embedding\n",
    "  * cf. [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/) for a beginner introduction to dense neural networks\n",
    "* a final sigmoid neuron that will output the probability for the review to be positive \n",
    "  * cf. [the 1-neuron network: logistic regression](https://thedatafrog.com/logistic-regression/) for a detailed discussion of the final sigmoid neuron.\n",
    "\n",
    "We start by creating an empty model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is the embedding layer. Its role is to convert each integer representing a word into a vector in N-dimensional space. In this space, words with similar meanings will be grouped together.\n",
    "\n",
    "Following the [keras documentation](https://keras.io/layers/embeddings/), we indicate the number of possible words, the dimension of the embedding space, and the maximum size of the text. \n",
    "\n",
    "We start with a 2-dimensional embedding space, as we had done in [word embedding and simple sentiment analysis](https://thedatafrog.com/word-embedding-sentiment-analysis). That's a very low number of dimensions. In fact, typically, embedding is done in 10-100 dimensions. \n",
    "\n",
    "But as usual, it's good to start small. We will try and increase the number of dimensions of the embedding space later to see if it improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_length = len(x_train[0])\n",
    "model.add(keras.layers.Embedding(len(vocab.words), 2, \n",
    "                                 input_length=review_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the embedding is multidimensional. Indeed, we start with a 1D array with 250 words. Since embedding gives us a two-dimensional vector for each word, the embedding layer spits out an array of shape (250, 2). This 2D array cannot be used directly as input to a dense layer, so we need to flatten it into a 1D array with 500 slots. This is done by the Flatten layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we add dropout regularization. In a nutshell, the dropout regularization layer drops, on a random basis, a fraction of its input values. This forces the network to learn different paths to solve the problem, and helps reduce [overfitting](https://thedatafrog.com/overfitting-illustrated/). If you want to know a bit more about dropout regularization, [check this out](https://thedatafrog.com/deep-learning-keras/#Dropout-layers). \n",
    "\n",
    "Here we decide to drop 40% of the values from the Flatten layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dropout(rate=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can add a dense layer, which will analyze the results of the embedding. Again, we start small, with only 5 neurons. We will see later if performance can be improved by increasing the number of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we end with a dense layer consisting of a single neuron with a [sigmoid activation function](https://thedatafrog.com/logistic-regression/). Therefore, this neuron will produce a value between 0 and 1, which is the estimated probability for the example review to be positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compile and print the full model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model on the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we end up with a validation accuracy of about 90%. To have a look at the performance in more details, we will use the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_accuracy(history, miny=None):\n",
    "    '''Plot the training and validation accuracy'''\n",
    "    acc = history.history['acc']\n",
    "    test_acc = history.history['val_acc']\n",
    "    epochs = range(len(acc))\n",
    "    plt.plot(epochs, acc)\n",
    "    plt.plot(epochs, test_acc)\n",
    "    if miny:\n",
    "        plt.ylim(miny, 1.0)\n",
    "    plt.title('accuracy') \n",
    "    plt.xlabel('epoch')\n",
    "    plt.figure()\n",
    "    \n",
    "plot_accuracy(history, miny=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy keeps increasing, but the testing accuracy plateaus. So we overfit. Let's increase the number of training examples. Each epoch will include more examples, and we can reduce the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "n_train = 1000000 # one million now\n",
    "x_test = x[:n_test]\n",
    "y_test = y[:n_test]\n",
    "x_train = x[n_test:n_train+n_test]\n",
    "y_train = y[n_test:n_train+n_test]\n",
    "\n",
    "# same network configuration as before: \n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(len(vocab.words), 2, \n",
    "                                 input_length=review_length))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(rate=0.4))\n",
    "model.add(keras.layers.Dense(5))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history,miny=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not overfit anymore. Now, the number of examples is 1 million. Therefore, at each epoch, the neural net is trained on these 1 million examples. We see that with a single epoch, the network is already well trained. After the second epoch (epoch 1), the training accuracy plateaus at 90%, so training further will not help. \n",
    "\n",
    "What we see here is that this network underfits the data, meaning that architecture is not complex enough to fit the data. By making it more complex, the training and testing accuracies can certainly be improved. A visual illustration of underfitting is shown [here](https://thedatafrog.com/overfitting-illustrated/#So-why-do-we-need-complex-networks-then?).\n",
    "\n",
    "## Dense network: increasing complexity\n",
    "\n",
    "After some tuning, I converged to the following architecture. The structure of the network is complex, so I use the full dataset to avoid overfitting. \n",
    "\n",
    "You can now execute the cell below and go grab a coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "x_test = x[:n_test]\n",
    "y_test = y[:n_test]\n",
    "x_train = x[n_test:]\n",
    "y_train = y[n_test:]\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(len(vocab.words), 128, # <<<<\n",
    "                                 input_length=review_length))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(rate=0.4))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history,miny=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "x_test = x[:n_test]\n",
    "y_test = y[:n_test]\n",
    "x_train = x[n_test:]\n",
    "y_train = y[n_test:]\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(len(vocab.words), 128, # <<<<\n",
    "                                 input_length=review_length))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(rate=0.4))\n",
    "model.add(keras.layers.Dense(250, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history,miny=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an improvement over the previous attempt, but the training is quite long, and it seems we will not be able to reach 93% classification accuracy on the test sample. In the next section we try a different strategy. \n",
    "\n",
    "## Sentiment analysis with a convolutional network\n",
    "\n",
    "We introduced convolutional layers when we tuned a [deep convolutional network for image recognition](https://thedatafrog.com/deep-learning-keras/). \n",
    "\n",
    "In this tutorial, the convolutional layer consists of a small window called the kernel which scans the image and extracts features at each position. The great advantage of convolutional layers for image recognition is that they can recognize parts of an image wherever these parts are in the image. Also, convolutional layers consider the pixels of the kernel together, which allows them to find local relationships between these pixels. \n",
    "\n",
    "In natural language processing, we deel with a sentence, not an image, but we can make the following analogies: \n",
    "\n",
    "* image: sentence\n",
    "* pixel: word\n",
    "* 2D convolution: 1D convolution\n",
    "\n",
    "In this section, we will introduce a 1D convolutional layer in our network, with a kernel size of 3. The following sentences illustrate how the convolutional layer will deal with a given review. At each step, the kernel moves by one word, and the words currently scanned by the kernel are indicated in boldface: \n",
    "\n",
    "* **this movie is** really not good\n",
    "* this **movie is really** not good\n",
    "* this movie **is really not** good\n",
    "* this movie is **really not good** \n",
    "\n",
    "**really not good** carries a lot of information for our sentiment analysis. The convolutional layer will find it whatever its position in the sentence. Also, it will be easy for the network to understand the meaning of **not good**. On the contrary, in our previous attempt, **not** and **good** are not directly considered together. \n",
    "\n",
    "Let's try. In the example below, the convolutional layer is set up with: \n",
    "\n",
    "* a kernel size of 3,\n",
    "* 64 filters. This means that 64 features (values) will be extracted from each position of the kernel,\n",
    "* a ReLU activation, as usual. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "n_train = 1000000\n",
    "x_test = x[:n_test]\n",
    "y_test = y[:n_test]\n",
    "x_train = x[n_test:n_test+n_train]\n",
    "y_train = y[n_test:n_test+n_train]\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(len(vocab.words), 64, input_length=250))\n",
    "model.add(keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(rate=0.4))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history,miny=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the convolutional layer, we can obtain almost the same performance as with our best try with a simple dense network. However, please note that: \n",
    "\n",
    "* there are only 2 million parameters in the network, instead of 10 million\n",
    "* the convolutional network is less subject to overfitting, and we could restrict the number of training examples to 1 million instead of 6.7 millions, and the training was much faster\n",
    "* there is room for optimization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked convolutional layers \n",
    "\n",
    "In this section, we will optimize our convolutional network further by stacking convolutional layers. \n",
    "\n",
    "As we have done in [Tuning a deep convolutional network for image recognition](https://thedatafrog.com/deep-learning-keras/), we perform **max pooling** between each convolutional layer, and the layers extract more and more features as we progress in the network. \n",
    "\n",
    "We use the whole dataset for training except for 20000 events to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "x_test = x[:n_test]\n",
    "y_test = y[:n_test]\n",
    "x_train = x[n_test:]\n",
    "y_train = y[n_test:]\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(len(vocab.words), 64, input_length=250))\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history,miny=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! 93.7% accuracy, and only a tiny bit of overfitting. There is probably some room for optimization, so please let us know in the comments if you manage to do better with convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the misclassified reviews\n",
    "\n",
    "It's always interesting to look at misclassified examples to get a hint of what's going on and maybe get ideas for further improvements. That's what we're going to do now, with the first 100 examples. \n",
    "\n",
    "Here are the predictions and the true labels for these samples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sample = x_test\n",
    "y_sample = y_test\n",
    "preds = model.predict_classes(x_sample)\n",
    "preds = np.array(preds).flatten()\n",
    "print('true:')\n",
    "print(y_sample)\n",
    "print('predictions:')\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we select the misclassified examples, together with the true label and the prediction for these examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = preds!=y_sample\n",
    "miscl = x_sample[idx]\n",
    "miscl_pred = preds[idx]\n",
    "miscl_true = y_sample[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we print the first five: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred, true, rev in zip(miscl_pred[:5], miscl_true, miscl[:5]):\n",
    "    rev = rev[rev!=0] # remove padding\n",
    "    print(pred, true)\n",
    "    print(' '.join(vocab.decode(rev)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too easy to understand the reviews with all the missing words, especially the stop words like \"the\", \"I\", \"a\", etc. Still, let's try. \n",
    "\n",
    "* 1st review: it seems that this person is comparing two bagel shops. She seems to like both and to refuse to compare them. Still, his rating is negative... \n",
    "* 2nd review: the text is clearly super positive but the rating is negative... \n",
    "* 3rd review: this person clearly states that this is not a review, and that she wants to ask a question about opening hours...\n",
    "* 4th review: this is a mixed review. I understand that the food is very good but that the restaurant is too expensive and that there were a few issues. The person still recommends to try it once. \n",
    "* 5th review: again a mixed review. \n",
    "\n",
    "So it appears we're not doing so bad: among the 5 misclassified reviews, three are weird. The last two ones correspond to borderline cases. \n",
    "\n",
    "We can build a pandas dataframe to look at the first 5 misclassified reviews. I know that these misclassified reviews are among the first 100 examples, so I will restrict the dataframe to this range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# take the first 4 columns of the first 100 examples. \n",
    "# give meaningful names to these columns\n",
    "df = pd.DataFrame(data=data[:100,:4], columns=['stars','useful','funny','cool'])\n",
    "# add a column to mark misclassified reviews: \n",
    "df['misc'] = idx[:100]\n",
    "# print the misclassified lines: \n",
    "df[df['misc']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't learn much, only that these reviews are indeed borderline: they have 3 or 4 stars, and we set the boundary between our negative and positive categories between 3 and 4 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
