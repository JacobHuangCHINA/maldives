{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Sentiment analysis is an important application of natural language processing, as it makes it possible to predict what a person thinks given the text she has written. \n",
    "\n",
    "For example, let's say you own a company and you would like to monitor the opinion of your customers on twitter. It's fairly easy to detect the tweets in which your company or products is mentioned, and to find out how many times these tweets are liked or retweeted. \n",
    "\n",
    "But tweets and likes do not mean that people like what you're doing! Maybe they're just destroying the reputation of your company online, or they like a funny tweet in which somebody says your products are really bad. \n",
    "\n",
    "That's where sentiment analysis is needed: it will tell you whether the tweet is positive or negative for your company, and how you should interpret all these likes and retweets.  \n",
    "\n",
    "This post is the fourth part of my tutorial series about natural language processing. You will learn how to classify the reviews of the large [yelp dataset](https://www.yelp.com/dataset) as positive or negative with two different deep neural networks.\n",
    "\n",
    "First, we will try a simple network consisting of an **embedding layer**, a **dense layer**, and a final **sigmoid neuron**. \n",
    "\n",
    "Then, we will see how **convolutional layers** can help us improve performance. \n",
    "\n",
    "If you don't know the terms used above, you can refer to these tutorials: \n",
    "\n",
    "* embedding is explained in [word embedding and simple sentiment analysis](https://thedatafrog.com/word-embedding-sentiment-analysis/);\n",
    "* dense layers are introduced in [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/);\n",
    "* the sigmoid neuron is described in details in [the 1-neuron network: logistic regression](https://thedatafrog.com/logistic-regression/);\n",
    "* convolutional layers are described at length in [tuning a deep convolutional network for image recognition, with keras and tensorflow](https://thedatafrog.com/deep-learning-keras/)\n",
    "\n",
    "Also, you will need to follow [this tutorial](https://thedatafrog.com/text-preprocessing-machine-learning-yelp/) to access the yelp dataset and preprocess it for machine learning. \n",
    "\n",
    "Finally, access to a GPU will save you a lot of time, as we want to train our neural networks on a large number of events here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get reproducible results\n",
    "from numpy.random import seed\n",
    "seed(0xdeadbeef)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(0xdeadbeef)\n",
    "\n",
    "# needed to run on a mac: \n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "datadir = '/data2/cbernet/maldives/yelp_dataset/'\n",
    "datafile = datadir+'data.h5'\n",
    "h5 = h5py.File(datafile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5['reviews']\n",
    "data = data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,1:]\n",
    "stars = data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 0 0] 6685900\n",
      "[5 3 5 ... 1 1 3] 6685900\n"
     ]
    }
   ],
   "source": [
    "y = np.zeros_like(stars)\n",
    "y[stars>3.5] = 1\n",
    "print(y, len(y))\n",
    "print(stars, len(stars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "n_train = 7000000\n",
    "x_test = x[:n_test]\n",
    "y_test = y[:n_test]\n",
    "x_train = x[n_test:n_train]\n",
    "y_train = y[n_test:n_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabulary import Vocabulary\n",
    "vocab = Vocabulary(dbfname=datadir+'/index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helped out when locked out apartment he quick got at price lowest comparison all other area definately recommend top master situations requiring locksmith they get job done quickly effectively'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev = x_test[0] # 1st test review\n",
    "rev = rev[rev!=0] # remove padding\n",
    "' '.join(vocab.decode(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3323.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        6677.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAExRJREFUeJzt3XGsnfV93/H3Jzika5vGJlwQss1MVbcLrZSEXYGrSF0bd8aQCfNHmBytw0XWPHUsardqK9n+YINGIps2WqQ2nVu8mqgNYWwdVsrKLEKUbRoEUygNUOQbQuHODLu9xl2Hko70uz/Oz8mB3Otzjn3vubn5vV/S1Xme7/N7zvP7+V7u5z6/5zkPqSokSf1522p3QJK0OgwASeqUASBJnTIAJKlTBoAkdcoAkKROjQyAJD+U5Kmhrz9L8nNJLkhyOMnR9rqhtU+Su5LMJXk6yRVD77WntT+aZM9KDkySdGaZ5HMASc4D/hdwFXAzsFBVdyS5BdhQVb+Q5Frgo8C1rd0vV9VVSS4AjgCzQAFPAH+9qk4u64gkSWOZdApoO/DlqvpjYBdwsNUPAte35V3APTXwKLA+ySXA1cDhqlpov/QPAzvPeQSSpLOybsL2u4FPt+WLq+oVgKp6JclFrb4ReHlon/lWW6q+pAsvvLC2bNkyYRclqW9PPPHEn1TVzKh2YwdAkvOB64CPjWq6SK3OUH/rcfYB+wAuvfRSjhw5Mm4XJUlAkj8ep90kU0DXAL9fVa+29Vfb1A7t9XirzwObh/bbBBw7Q/1Nqmp/Vc1W1ezMzMgAkySdpUkC4CN8c/oH4BBw+k6ePcADQ/Ub291A24BTbaroIWBHkg3tjqEdrSZJWgVjTQEl+W7gbwJ/f6h8B3Bfkr3AS8ANrf4ggzuA5oDXgZsAqmohye3A463dbVW1cM4jkCSdlYluA5222dnZ8hqAJE0myRNVNTuqnZ8ElqROGQCS1CkDQJI6ZQBIUqcMAEnq1KSPgpCkbmy55XdX7dgv3vGhFT+GZwCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE6NFQBJ1ie5P8kfJXkuyY8muSDJ4SRH2+uG1jZJ7koyl+TpJFcMvc+e1v5okj0rNShJ0mjjngH8MvB7VfXXgPcCzwG3AA9X1Vbg4bYOcA2wtX3tAz4JkOQC4FbgKuBK4NbToSFJmr6RAZDk+4AfA+4GqKq/qKrXgF3AwdbsIHB9W94F3FMDjwLrk1wCXA0crqqFqjoJHAZ2LutoJEljG+cM4PuBE8C/T/Jkkt9I8j3AxVX1CkB7vai13wi8PLT/fKstVZckrYJxAmAdcAXwyap6P/B/+eZ0z2KySK3OUH/zzsm+JEeSHDlx4sQY3ZMknY1xAmAemK+qx9r6/QwC4dU2tUN7PT7UfvPQ/puAY2eov0lV7a+q2aqanZmZmWQskqQJjAyAqvrfwMtJfqiVtgPPAoeA03fy7AEeaMuHgBvb3UDbgFNtiughYEeSDe3i745WkyStgnVjtvso8FtJzgdeAG5iEB73JdkLvATc0No+CFwLzAGvt7ZU1UKS24HHW7vbqmphWUYhSZrYWAFQVU8Bs4ts2r5I2wJuXuJ9DgAHJumgJGll+ElgSeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUqbECIMmLSf4wyVNJjrTaBUkOJznaXje0epLclWQuydNJrhh6nz2t/dEke1ZmSJKkcUxyBvATVfW+qppt67cAD1fVVuDhtg5wDbC1fe0DPgmDwABuBa4CrgRuPR0akqTpO5cpoF3AwbZ8ELh+qH5PDTwKrE9yCXA1cLiqFqrqJHAY2HkOx5cknYNxA6CA/5rkiST7Wu3iqnoFoL1e1OobgZeH9p1vtaXqkqRVsG7Mdh+oqmNJLgIOJ/mjM7TNIrU6Q/3NOw8CZh/ApZdeOmb3JEmTGusMoKqOtdfjwO8wmMN/tU3t0F6Pt+bzwOah3TcBx85Qf+ux9lfVbFXNzszMTDYaSdLYRgZAku9J8s7Ty8AO4EvAIeD0nTx7gAfa8iHgxnY30DbgVJsiegjYkWRDu/i7o9UkSatgnCmgi4HfSXK6/W9X1e8leRy4L8le4CXghtb+QeBaYA54HbgJoKoWktwOPN7a3VZVC8s2EknSREYGQFW9ALx3kfqfAtsXqRdw8xLvdQA4MHk3JUnLzU8CS1KnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTo0dAEnOS/Jkks+29cuSPJbkaJLPJDm/1d/R1ufa9i1D7/GxVn8+ydXLPRhJ0vgmOQP4WeC5ofVPAHdW1VbgJLC31fcCJ6vqB4A7WzuSXA7sBn4Y2An8apLzzq37kqSzNVYAJNkEfAj4jbYe4IPA/a3JQeD6tryrrdO2b2/tdwH3VtXXquorwBxw5XIMQpI0uXHPAH4J+KfAX7b1dwOvVdUbbX0e2NiWNwIvA7Ttp1r7b9QX2UeSNGUjAyDJ3wKOV9UTw+VFmtaIbWfaZ/h4+5IcSXLkxIkTo7onSTpL45wBfAC4LsmLwL0Mpn5+CVifZF1rswk41pbngc0Abfu7gIXh+iL7fENV7a+q2aqanZmZmXhAkqTxjAyAqvpYVW2qqi0MLuJ+rqr+DvAI8OHWbA/wQFs+1NZp2z9XVdXqu9tdQpcBW4EvLttIJEkTWTe6yZJ+Abg3yS8CTwJ3t/rdwKeSzDH4y383QFU9k+Q+4FngDeDmqvr6ORxfknQOJgqAqvo88Pm2/AKL3MVTVV8Fblhi/48DH5+0k5Kk5ecngSWpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTqX/yPYt70tt/zuqhz3xTs+tCrHlaRJeAYgSZ0yACSpUwaAJHXKAJCkThkAktSpkQGQ5LuSfDHJHyR5Jsm/bPXLkjyW5GiSzyQ5v9Xf0dbn2vYtQ+/1sVZ/PsnVKzUoSdJo45wBfA34YFW9F3gfsDPJNuATwJ1VtRU4Cext7fcCJ6vqB4A7WzuSXA7sBn4Y2An8apLzlnMwkqTxjQyAGvjztvr29lXAB4H7W/0gcH1b3tXWadu3J0mr31tVX6uqrwBzwJXLMgpJ0sTGugaQ5LwkTwHHgcPAl4HXquqN1mQe2NiWNwIvA7Ttp4B3D9cX2UeSNGVjBUBVfb2q3gdsYvBX+3sWa9Zes8S2pepvkmRfkiNJjpw4cWKc7kmSzsJEdwFV1WvA54FtwPokpx8lsQk41pbngc0Abfu7gIXh+iL7DB9jf1XNVtXszMzMJN2TJE1gnLuAZpKsb8t/BfhJ4DngEeDDrdke4IG2fKit07Z/rqqq1Xe3u4QuA7YCX1yugUiSJjPOw+AuAQ62O3beBtxXVZ9N8ixwb5JfBJ4E7m7t7wY+lWSOwV/+uwGq6pkk9wHPAm8AN1fV15d3OJKkcY0MgKp6Gnj/IvUXWOQunqr6KnDDEu/1ceDjk3dTkrTc/CSwJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnq1MgASLI5ySNJnkvyTJKfbfULkhxOcrS9bmj1JLkryVySp5NcMfRee1r7o0n2rNywJEmjjHMG8Abw81X1HmAbcHOSy4FbgIeraivwcFsHuAbY2r72AZ+EQWAAtwJXAVcCt54ODUnS9I0MgKp6pap+vy3/H+A5YCOwCzjYmh0Erm/Lu4B7auBRYH2SS4CrgcNVtVBVJ4HDwM5lHY0kaWwTXQNIsgV4P/AYcHFVvQKDkAAuas02Ai8P7TbfakvVJUmrYOwASPK9wH8Efq6q/uxMTRep1Rnqbz3OviRHkhw5ceLEuN2TJE1orABI8nYGv/x/q6r+Uyu/2qZ2aK/HW30e2Dy0+ybg2Bnqb1JV+6tqtqpmZ2ZmJhmLJGkC49wFFOBu4Lmq+rdDmw4Bp+/k2QM8MFS/sd0NtA041aaIHgJ2JNnQLv7uaDVJ0ipYN0abDwB/F/jDJE+12j8D7gDuS7IXeAm4oW17ELgWmANeB24CqKqFJLcDj7d2t1XVwrKMQpI0sZEBUFX/ncXn7wG2L9K+gJuXeK8DwIFJOihJWhl+EliSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHVqZAAkOZDkeJIvDdUuSHI4ydH2uqHVk+SuJHNJnk5yxdA+e1r7o0n2rMxwJEnjGucM4DeBnW+p3QI8XFVbgYfbOsA1wNb2tQ/4JAwCA7gVuAq4Erj1dGhIklbHyACoqi8AC28p7wIOtuWDwPVD9Xtq4FFgfZJLgKuBw1W1UFUngcN8a6hIkqbobK8BXFxVrwC014tafSPw8lC7+VZbqi5JWiXLfRE4i9TqDPVvfYNkX5IjSY6cOHFiWTsnSfqmsw2AV9vUDu31eKvPA5uH2m0Cjp2h/i2qan9VzVbV7MzMzFl2T5I0ytkGwCHg9J08e4AHhuo3truBtgGn2hTRQ8COJBvaxd8drSZJWiXrRjVI8mngx4ELk8wzuJvnDuC+JHuBl4AbWvMHgWuBOeB14CaAqlpIcjvweGt3W1W99cKyJGmKRgZAVX1kiU3bF2lbwM1LvM8B4MBEvZMkrRg/CSxJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjo19QBIsjPJ80nmktwy7eNLkgamGgBJzgN+BbgGuBz4SJLLp9kHSdLAtM8ArgTmquqFqvoL4F5g15T7IEli+gGwEXh5aH2+1SRJU7ZuysfLIrV6U4NkH7Cvrf55kufP4XgXAn9yDvuflXxi2kf8hlUZ7ypzzH3obsz5xDmN+a+O02jaATAPbB5a3wQcG25QVfuB/ctxsCRHqmp2Od5rLehtvOCYe+GYV8a0p4AeB7YmuSzJ+cBu4NCU+yBJYspnAFX1RpJ/CDwEnAccqKpnptkHSdLAtKeAqKoHgQendLhlmUpaQ3obLzjmXjjmFZCqGt1KkvQdx0dBSFKn1nwAjHq0RJJ3JPlM2/5Yki3T7+XyGmPM/zjJs0meTvJwkrFuCft2Nu4jRJJ8OEklWfN3jIwz5iR/u32vn0ny29Pu43Ib42f70iSPJHmy/Xxfuxr9XC5JDiQ5nuRLS2xPkrvav8fTSa5Y1g5U1Zr9YnAh+cvA9wPnA38AXP6WNv8A+LW2vBv4zGr3ewpj/gngu9vyz/Qw5tbuncAXgEeB2dXu9xS+z1uBJ4ENbf2i1e73FMa8H/iZtnw58OJq9/scx/xjwBXAl5bYfi3wXxh8hmob8NhyHn+tnwGM82iJXcDBtnw/sD3JYh9IWytGjrmqHqmq19vqoww+b7GWjfsIkduBfwV8dZqdWyHjjPnvAb9SVScBqur4lPu43MYZcwHf15bfxVs+R7TWVNUXgIUzNNkF3FMDjwLrk1yyXMdf6wEwzqMlvtGmqt4ATgHvnkrvVsakj9PYy+AviLVs5JiTvB/YXFWfnWbHVtA43+cfBH4wyf9I8miSnVPr3coYZ8z/AvipJPMM7ib86HS6tmpW9PE5U78NdJmNfLTEmG3WkrHHk+SngFngb6xoj1beGcec5G3AncBPT6tDUzDO93kdg2mgH2dwlvffkvxIVb22wn1bKeOM+SPAb1bVv0nyo8Cn2pj/cuW7typW9PfXWj8DGPloieE2SdYxOG080ynXt7txxkySnwT+OXBdVX1tSn1bKaPG/E7gR4DPJ3mRwVzpoTV+IXjcn+0Hqur/VdVXgOcZBMJaNc6Y9wL3AVTV/wS+i8Fzgr5TjfXf+9la6wEwzqMlDgF72vKHgc9Vu7qyRo0cc5sO+XcMfvmv9XlhGDHmqjpVVRdW1Zaq2sLgusd1VXVkdbq7LMb52f7PDC74k+RCBlNCL0y1l8trnDG/BGwHSPIeBgFwYqq9nK5DwI3tbqBtwKmqemW53nxNTwHVEo+WSHIbcKSqDgF3MzhNnGPwl//u1evxuRtzzP8a+F7gP7Tr3S9V1XWr1ulzNOaYv6OMOeaHgB1JngW+DvyTqvrT1ev1uRlzzD8P/HqSf8RgKuSn1/IfdEk+zWAK78J2XeNW4O0AVfVrDK5zXAvMAa8DNy3r8dfwv50k6Rys9SkgSdJZMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASerU/wdcvqIMHH+QoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_test[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 250, 64)           1280128   \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,880,329\n",
      "Trainable params: 2,880,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# the first layer is the embedding layer. \n",
    "# we indicate the number of possible words, \n",
    "# the dimension of the embedding space, \n",
    "# and the maximum size of the text. \n",
    "model.add(keras.layers.Embedding(len(vocab.words), 64, input_length=250))\n",
    "\n",
    "# the output of the embedding is multidimensional, \n",
    "# with shape (256, 2)\n",
    "# for each word, we obtain two values, \n",
    "# the x and y coordinates\n",
    "# we flatten this output to be able to \n",
    "# use it in a dense layer\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "# dropout regularization\n",
    "model.add(keras.layers.Dropout(rate=0.4))\n",
    "\n",
    "# small dense layer. It's role is to analyze \n",
    "# the distribution of points from embedding\n",
    "model.add(keras.layers.Dense(100))\n",
    "\n",
    "# final neuron, with sigmoid activation \n",
    "# for binary classification\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6665900 samples, validate on 20000 samples\n",
      "Epoch 1/2\n",
      "6665900/6665900 [==============================] - 115s 17us/step - loss: 0.2377 - acc: 0.9056 - val_loss: 0.2204 - val_acc: 0.9126\n",
      "Epoch 2/2\n",
      "6665900/6665900 [==============================] - 115s 17us/step - loss: 0.2270 - acc: 0.9107 - val_loss: 0.2200 - val_acc: 0.9139\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=2,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample = x_test[:100]\n",
    "y_sample = y_test[:100]\n",
    "preds = model.predict_classes(x_sample)\n",
    "preds = np.array(preds).flatten()\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1], dtype=int16)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = preds!=y_sample\n",
    "miscl = x_sample[idx]\n",
    "miscl_pred = preds[idx]\n",
    "miscl_true = y_sample[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0\n",
      "on wet snowy sunday afternoon made trek out famous fairmount bagels since st. close decided pick up some from there as well compare each other as well as bagels all name research people ! fairmount vs. st. bagels from both locations were fresh from oven still warm when they were handed us ca n't say preferred one over other eventually bagels from both locations were mixed up could n't pick out which ones came from where they were also both great with cream cheese & smoked salmon montreal vs. ny : both respectable their own way ny counterparts & thicker can taken more seriously as meal montreal bagels thinner with more crust crumb ratio best eaten as snack love snacks overall enjoy all bagels refuse play this silly game favorites\n",
      "\n",
      "\n",
      "1 0\n",
      "this hotel mixed bag 5 stars chandelier 2 stars layout 4 stars location on strip 4 stars rooms 2 stars cleanliness chandelier all on its own everyone has take at least one picture front gorgeous hotel overall classy chic very modern personally n't like layout hotel casino 's confusing not conducive after late night drinking might get lost hotel 's location on strip perfect smacked dab middle las vegas blvd only problem parking garage taxi stand areas small stuffy will have wait long time cabs because they n't have large area drive into rooms nice comfortable saw rooms on lower floors they were very cute had cute sitting area very homey feel decor differs on certain floors also on 60th floor these rooms were amazing ! roomy ultra clean ultra modern everything top line this where celebrities stay like rooms just n't like drastic difference decor among all rooms this high end hotel should have high end rooms no matter what cost level cleanliness kind goes with rooms thought rooms were thoroughly clean but places outside hotel needed good scrub down hallways lower level floors looked horrible ( see picture ) they were grey dirty again high end hotel should have high end everywhere overall 's good hotel but price pay expect highest levels cleanliness experience\n",
      "\n",
      "\n",
      "1 0\n",
      "went this little cozy restaurant with wife quiet intimate romantic resto with attentive owner unpretentious capable staff had crab croquettes caesar salad had salmon ( delicious ! crispy on outside perfectly flaky on inside ) desserts : tiramisu wife had chocolate brownie with ice cream very fine tasty meal\n",
      "\n",
      "\n",
      "1 0\n",
      "happy when found out they have fish company at downtown food not as fresh as one stripe like one stripe better even they have same menu\n",
      "\n",
      "\n",
      "0 1\n",
      "this not review as such hope try this place on next visit vegas but question opening hours on yelp listed as 11:00 - however people have left reviews stating they have visited after midnight which when will probably visiting does anyone know actual opening hours ? thanks\n",
      "\n",
      "\n",
      "0 1\n",
      "came here brunch on sunday only buffet around $ 50 person plus tax tips get unlimited coffee tea juice - orange grapefruit or mixed there lobster but when sat down mostly gone they were did not refill missed out on lobster tails as saw people grabbing 2-3 each on their plates did get some lobster from claws ( all taken out by worker sat on bowl ) french toast great as well all others just so-so service great they your napkin each time left brought new fork spoons still n't think worth money thou good try once done with\n",
      "\n",
      "\n",
      "0 1\n",
      "personally have not been blow away by food here 's decent small chinese restaurant like beef rolls they mildly sweet definitely one highlights did n't like their specialty pan fried buns too much because they tasted little bitter but they were kind enough make only pork buns since am allergic shrimp had their pork chop rice which little bland may have try beef noodles next time reason gave 4 stars because dad loves this place he raves about combination noodle bowl saying how nostalgic made him feel they make their own cat ear noodles with were pretty good service as good as 'd expect chinese restaurant ...\n",
      "\n",
      "\n",
      "0 1\n",
      "sushi lovers out there ... n't too bad get mixed roll or aka burrito not big burrito more like decent size sushi make roll but like variety items got into roll only first time fyi may need soy sauce fish decently fresh taro chips with cheese sauce yum ca n't remember name rolls tried but will update review later fyi they have frequent buyer too !\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pred, true, rev in zip(miscl_pred, miscl_true, miscl):\n",
    "    rev = rev[rev!=0] # remove padding\n",
    "    print(pred, true)\n",
    "    print(' '.join(vocab.decode(rev)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 250, 64)           1280128   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 250, 16)           3088      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 125, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 125, 32)           1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 62, 64)            6208      \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 100)               396900    \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,687,993\n",
      "Trainable params: 1,687,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# the first layer is the embedding layer. \n",
    "# we indicate the number of possible words, \n",
    "# the dimension of the embedding space, \n",
    "# and the maximum size of the text. \n",
    "model.add(keras.layers.Embedding(len(vocab.words), 64, input_length=250))\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "# the output of the embedding is multidimensional, \n",
    "# with shape (256, 2)\n",
    "# for each word, we obtain two values, \n",
    "# the x and y coordinates\n",
    "# we flatten this output to be able to \n",
    "# use it in a dense layer\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "# dropout regularization\n",
    "model.add(keras.layers.Dropout(rate=0.4))\n",
    "\n",
    "# small dense layer. It's role is to analyze \n",
    "# the distribution of points from embedding\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "# final neuron, with sigmoid activation \n",
    "# for binary classification\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6665900 samples, validate on 20000 samples\n",
      "Epoch 1/4\n",
      "6665900/6665900 [==============================] - 138s 21us/step - loss: 0.1827 - acc: 0.9271 - val_loss: 0.1641 - val_acc: 0.9348\n",
      "Epoch 2/4\n",
      "6665900/6665900 [==============================] - 137s 21us/step - loss: 0.1590 - acc: 0.9370 - val_loss: 0.1592 - val_acc: 0.9358\n",
      "Epoch 3/4\n",
      "6665900/6665900 [==============================] - 137s 21us/step - loss: 0.1500 - acc: 0.9407 - val_loss: 0.1604 - val_acc: 0.9374\n",
      "Epoch 4/4\n",
      "6665900/6665900 [==============================] - 137s 21us/step - loss: 0.1433 - acc: 0.9436 - val_loss: 0.1601 - val_acc: 0.9381\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 250, 64)           1280128   \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 250, 16)           5136      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 125, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 125, 32)           2080      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 62, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 31, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 100)               198500    \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,492,153\n",
      "Trainable params: 1,492,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# the first layer is the embedding layer. \n",
    "# we indicate the number of possible words, \n",
    "# the dimension of the embedding space, \n",
    "# and the maximum size of the text. \n",
    "model.add(keras.layers.Embedding(len(vocab.words), 64, input_length=250))\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters=16, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "# the output of the embedding is multidimensional, \n",
    "# with shape (256, 2)\n",
    "# for each word, we obtain two values, \n",
    "# the x and y coordinates\n",
    "# we flatten this output to be able to \n",
    "# use it in a dense layer\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "# dropout regularization\n",
    "model.add(keras.layers.Dropout(rate=0.4))\n",
    "\n",
    "# small dense layer. It's role is to analyze \n",
    "# the distribution of points from embedding\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "# final neuron, with sigmoid activation \n",
    "# for binary classification\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6665900 samples, validate on 20000 samples\n",
      "Epoch 1/4\n",
      "6665900/6665900 [==============================] - 146s 22us/step - loss: 0.1818 - acc: 0.9273 - val_loss: 0.1615 - val_acc: 0.9356\n",
      "Epoch 2/4\n",
      "6665900/6665900 [==============================] - 146s 22us/step - loss: 0.1575 - acc: 0.9377 - val_loss: 0.1580 - val_acc: 0.9389\n",
      "Epoch 3/4\n",
      "6665900/6665900 [==============================] - 146s 22us/step - loss: 0.1476 - acc: 0.9419 - val_loss: 0.1580 - val_acc: 0.9372\n",
      "Epoch 4/4\n",
      "6665900/6665900 [==============================] - 146s 22us/step - loss: 0.1393 - acc: 0.9454 - val_loss: 0.1594 - val_acc: 0.9376\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 250, 64)           1280128   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 250, 16)           3088      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 125, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 125, 32)           1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 62, 64)            6208      \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 3968)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 100)               396900    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,687,993\n",
      "Trainable params: 1,687,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6665900 samples, validate on 20000 samples\n",
      "Epoch 1/4\n",
      "6665900/6665900 [==============================] - 138s 21us/step - loss: 0.1821 - acc: 0.9271 - val_loss: 0.1653 - val_acc: 0.9333\n",
      "Epoch 2/4\n",
      "6665900/6665900 [==============================] - 138s 21us/step - loss: 0.1595 - acc: 0.9368 - val_loss: 0.1596 - val_acc: 0.9353\n",
      "Epoch 3/4\n",
      "6665900/6665900 [==============================] - 138s 21us/step - loss: 0.1509 - acc: 0.9404 - val_loss: 0.1594 - val_acc: 0.9369\n",
      "Epoch 4/4\n",
      "6665900/6665900 [==============================] - 138s 21us/step - loss: 0.1442 - acc: 0.9432 - val_loss: 0.1613 - val_acc: 0.9355\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# the first layer is the embedding layer. \n",
    "# we indicate the number of possible words, \n",
    "# the dimension of the embedding space, \n",
    "# and the maximum size of the text. \n",
    "model.add(keras.layers.Embedding(len(vocab.words), 64, input_length=250))\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "# the output of the embedding is multidimensional, \n",
    "# with shape (256, 2)\n",
    "# for each word, we obtain two values, \n",
    "# the x and y coordinates\n",
    "# we flatten this output to be able to \n",
    "# use it in a dense layer\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "# dropout regularization\n",
    "model.add(keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "# small dense layer. It's role is to analyze \n",
    "# the distribution of points from embedding\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "# final neuron, with sigmoid activation \n",
    "# for binary classification\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=1000,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
