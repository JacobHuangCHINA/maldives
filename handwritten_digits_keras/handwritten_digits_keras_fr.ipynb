{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## À propos de ce tutoriel \n",
    "\n",
    "Dans mon article sur la [reconnaissance de chiffres manuscrits avec scikit-learn](https://thedatafrog.com/fr/reconnaissance-decriture-manuscrite-scikit-learn/), nous avons vu qu'avec [scikit-learn](https://scikit-learn.org), il est facile d'entraîner un réseau de neurones simple. Nous avons réussi à reconnaître des images de chiffres avec une précision supérieure à 90%. \n",
    "\n",
    "Malheureusement, scikit-learn n'est généralement pas adapté aux réseaux de neurones. \n",
    "\n",
    "Son but est en fait de fournir une interface unifiée pour l'entraînement et le test de différents algorithmes de machine learning: réseaux de neurones bien sûr, mais aussi SVM, Bayes, plus proches voisins, arbres de décision, etc. \n",
    "\n",
    "En effet, dans un projet de machine learning, on passe une bonne partie de son temps à choisir le bon algorithme et à l'optimiser. Scikit-learn a été conçu pour rendre ces tâches aussi simples et rapides que possible. \n",
    "\n",
    "Par contre, pour ce qui est des réseaux de neurones: \n",
    "\n",
    "* son interface ne permet pas de créer de réseaux complexes, ni de contrôler leur fonctionnement dans les détails \n",
    "* il n'est pas adapté au [deep learning](https://thedatafrog.com/install-tensorflow-windows/)\n",
    "\n",
    "Dans cet article, nous allons répéter notre exercice sur la reconnaissance de chiffres manuscrits, en utilisant cette fois [Keras](https://keras.io/), un package spécialisé dans les réseaux de neurones. \n",
    "\n",
    "Vous apprendrez à:\n",
    "\n",
    "* Installer Keras\n",
    "* Créer un réseau de neurones simple avec cet outil\n",
    "* Estimer les performances de ce réseau\n",
    "\n",
    "À l'avenir, nous utiliserons Keras comme interface vers TensorFlow pour faire du deep learning. \n",
    "\n",
    "**Prérequis:** \n",
    "\n",
    "Il faudrait que vous ayez déjà manipulé un réseau de neurones simple et que vous ayez quelques notions de: \n",
    "\n",
    "* numpy, \n",
    "* matplotlib, \n",
    "* jupyter notebook\n",
    "\n",
    "Si ce n'est pas le cas, le tuto sur la [reconnaissance de chiffres manuscrits avec scikit-learn](https://thedatafrog.com/fr/reconnaissance-decriture-manuscrite-scikit-learn/) est fait pour vous!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Installez d'abord [Anaconda](https://www.anaconda.com/download/) si ce n'est pas encore fait. Choisissez bien **la version pour python 2.X, et pas pour python 3.X**.\n",
    "\n",
    "Lancez l'application Anaconda Navigator. \n",
    "\n",
    "Dans l'onglet Environments, sélectionnez l'environnement de base (root), \n",
    "et installez-y le package keras. Anaconda peut mettre plusieurs minutes à installer ce package, soyez patient.\n",
    "\n",
    "Ensuite, allez dans l'onglet Home pour lancer jupyter notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, récupérez ce notebook, et ouvrez-le dans jupyter notebook.\n",
    "\n",
    "* [téléchargez le dépôt contenant ce notebook](https://github.com/cbernet/maldives/archive/master.zip)\n",
    "* décompressez-le, par exemple dans `Downloads/maldives-master`\n",
    "* dans jupyter notebook, naviguez vers `Downloads/maldives-master/handwritten_digits_keras`\n",
    "* ouvrez `handwritten_digits_keras_fr.ipynb`\n",
    "\n",
    "Cette page devrait apparaître dans le notebook. À partir de maintenant, vous pouvez suivre ce tutoriel dans le notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation de l'échantillon de données\n",
    "\n",
    "Comme dans [reconnaissance de chiffres manuscrits avec scikit-learn](https://thedatafrog.com/fr/reconnaissance-decriture-manuscrite-scikit-learn/), nous allons utiliser l'échantillon de chiffres fourni avec scikit-learn. Les chiffres sont des images 8x8, et nous allons les traiter avec un réseau de neurones avec: \n",
    "\n",
    "* une couche d'entrée avec 8x8 = 64 neurones \n",
    "* une couche cachée de 15 neurones \n",
    "* une couche de sortie de 10 neurones, correspondant aux dix catégories de chiffres. \n",
    "\n",
    "D'abord, initialisons nos outils et chargeons l'échantillon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "# for some reason, the following \n",
    "# is needed to run on mac os X\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'  \n",
    "\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La couche d'entrée requiert un tableau 1D de 64 valeurs, mais nos images sont 2D, avec 8x8 pixels. Nous devons donc les sérialiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, un peu de travail est nécessaire avant de pouvoir utiliser les étiquettes. \n",
    "\n",
    "Pour l'instant, `digits.target` contient le chiffre auquel correspond chaque image dans l'échantillon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais avec Keras, nous devons construire un réseau avec 10 neurones de sortie. C'est aussi le cas avec scikit-learn, bien que ce soit caché à l'utilisateur. Au cours de l'entraînement, Keras devra comparer les 10 valeurs de sortie de ces neurones à l'étiquette pour donner un retour au réseau et lui permettre de s'améliorer. Mais comment pouvons nous comparer un tableau de 10 valeurs à une seule valeur, celle de l'étiquette?\n",
    "\n",
    "La solution est de traduire chaque étiquette en un tableau de taille 10 (une technique appelée *one-hot encoding*): \n",
    "\n",
    "* la valeur `0` donne `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
    "* `1` donne `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
    "* ...\n",
    "* `9` donne `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`\n",
    "\n",
    "Après cela, les valeurs des 10 neurones de sortie, qui sont des probabilités entre 0 et 1, peuvent être directement comparées aux 10 valeurs de l'étiquette. \n",
    "\n",
    "De cette façon, pour un chiffre donné, par exemple 0, le réseau de neurones sera entraîné à donner une probabilité élevée sur le premier neurone de sortie, et une faible probabilité sur les neurones suivants. \n",
    "\n",
    "Cet encodage se fait facilement avec les outils fournis par Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(digits.target,10)\n",
    "print digits.target\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant partager nos données entre un échantillon d'entraînement et un échantillon de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_limit=1000\n",
    "x_train = x[:split_limit]\n",
    "y_train = y[:split_limit]\n",
    "x_test = x[split_limit:]\n",
    "y_test = y[split_limit:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 1000 premières images et étiquettes seront utilisées pour l'entraînement, et les suivantes pour l'évaluation des performances de notre réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du réseau de neurones avec Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir importé les outils nécessaires, créons le réseau de neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Model, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                975       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                160       \n",
      "=================================================================\n",
      "Total params: 1,135\n",
      "Trainable params: 1,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Création de la couche d'entrée\n",
    "# \n",
    "# On spécifie que cette couche doit \n",
    "# avoir 64 neurones, un pour chaque \n",
    "# pixel dans nos images. \n",
    "# Les neurones d'entrée ne font rien, \n",
    "# ils se contentent de transférer la \n",
    "# valeur sur chaque pixel à la couche \n",
    "# suivante.\n",
    "img_input = layers.Input(shape=(64,))\n",
    "\n",
    "\n",
    "# Création de la couche cachée\n",
    "#\n",
    "# Cette couche est dense, ce qui veut \n",
    "# dire que chacun de ses neurones est \n",
    "# connecté à tous les neurones de la \n",
    "# couche précédente (la couche d'entrée).\n",
    "# Nous parlerons de l'activation dans un\n",
    "# futur post.\n",
    "tmp = layers.Dense(15, \n",
    "                   activation='sigmoid')(img_input)\n",
    "\n",
    "# Création de la couche de sortie\n",
    "# \n",
    "# La couche de sortie est dense également. \n",
    "# Elle doit avoir 10 neurones, correspondant\n",
    "# aux 10 catégories de chiffres. \n",
    "output = layers.Dense(10, \n",
    "                      activation='sigmoid')(tmp)\n",
    "\n",
    "# Création du réseau de neurones \n",
    "# à partir des couches\n",
    "model = Model(img_input, output)\n",
    "\n",
    "# Impression d'une description du réseau\n",
    "model.summary()\n",
    "\n",
    "# =================================================\n",
    "# Merci de pas prêter attention à cette partie. \n",
    "# Nous parlerons de la régularisation plus tard.\n",
    "# Pour l'instant, il suffit de noter que la \n",
    "# régularisation aide le réseau à converger \n",
    "# correctement. \n",
    "# J'ai rajouté cette régularisation ici car elle \n",
    "# est effectuée par défaut dans scikit-learn,  \n",
    "# et que nous voulons pouvoir comparer les résultats\n",
    "# de keras et scikit-learn. \n",
    "l2_rate = 1e-4\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer = regularizers.l2(l2_rate)\n",
    "        layer.bias_regularizer = regularizers.l2(l2_rate)\n",
    "        layer.activity_regularizer = regularizers.l2(l2_rate)\n",
    "# =================================================\n",
    "\n",
    "# Définition de la méthode d'apprentissage, \n",
    "# et compilation du modèle.\n",
    "# \n",
    "# Le modèle doit être compilé pour être \n",
    "# entraîné et utilisé. \n",
    "# Les arguments loss, optimizer, et metric\n",
    "# seront couverts dans un futur post. \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.1, momentum=0.9),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, nous pouvons entraîner le réseau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 797 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s 466us/step - loss: 2.2659 - acc: 0.1090 - val_loss: 2.1448 - val_acc: 0.2560\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.0436 - acc: 0.4730 - val_loss: 1.9326 - val_acc: 0.6035\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 1.8035 - acc: 0.6970 - val_loss: 1.6691 - val_acc: 0.7327\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 1.4918 - acc: 0.7700 - val_loss: 1.3363 - val_acc: 0.7992\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 1.1356 - acc: 0.8410 - val_loss: 1.0526 - val_acc: 0.8030\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.8624 - acc: 0.8750 - val_loss: 0.8443 - val_acc: 0.8444\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.6668 - acc: 0.9070 - val_loss: 0.6984 - val_acc: 0.8469\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.5123 - acc: 0.9240 - val_loss: 0.5705 - val_acc: 0.8858\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.4076 - acc: 0.9510 - val_loss: 0.5100 - val_acc: 0.8971\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.3437 - acc: 0.9630 - val_loss: 0.4981 - val_acc: 0.8871\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.2983 - acc: 0.9650 - val_loss: 0.4611 - val_acc: 0.8883\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s 61us/step - loss: 0.2726 - acc: 0.9630 - val_loss: 0.4857 - val_acc: 0.8758\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.2361 - acc: 0.9680 - val_loss: 0.4413 - val_acc: 0.8921\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 0.2084 - acc: 0.9770 - val_loss: 0.3789 - val_acc: 0.9097\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1744 - acc: 0.9820 - val_loss: 0.3751 - val_acc: 0.8971\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s 67us/step - loss: 0.1442 - acc: 0.9910 - val_loss: 0.3625 - val_acc: 0.9072\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.1288 - acc: 0.9900 - val_loss: 0.3482 - val_acc: 0.9122\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.1138 - acc: 0.9920 - val_loss: 0.3232 - val_acc: 0.9122\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.1057 - acc: 0.9940 - val_loss: 0.3526 - val_acc: 0.9059\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.1039 - acc: 0.9930 - val_loss: 0.3061 - val_acc: 0.9197\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0923 - acc: 0.9950 - val_loss: 0.3084 - val_acc: 0.9147\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 0.0917 - acc: 0.9950 - val_loss: 0.3140 - val_acc: 0.9072\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.1009 - acc: 0.9880 - val_loss: 0.3621 - val_acc: 0.8959\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.1089 - acc: 0.9820 - val_loss: 0.3835 - val_acc: 0.9021\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.0971 - acc: 0.9860 - val_loss: 0.3472 - val_acc: 0.8984\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.0829 - acc: 0.9940 - val_loss: 0.3245 - val_acc: 0.9134\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0794 - acc: 0.9910 - val_loss: 0.3275 - val_acc: 0.9097\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0735 - acc: 0.9940 - val_loss: 0.3263 - val_acc: 0.9084\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0624 - acc: 0.9950 - val_loss: 0.3376 - val_acc: 0.9109\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.0653 - acc: 0.9920 - val_loss: 0.3233 - val_acc: 0.9097\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.0549 - acc: 0.9950 - val_loss: 0.3103 - val_acc: 0.9109\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0557 - acc: 0.9960 - val_loss: 0.3095 - val_acc: 0.9159\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.0476 - acc: 0.9980 - val_loss: 0.3108 - val_acc: 0.9072\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0422 - acc: 0.9990 - val_loss: 0.3048 - val_acc: 0.9159\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0396 - acc: 0.9990 - val_loss: 0.3058 - val_acc: 0.9097\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 0.0378 - acc: 0.9990 - val_loss: 0.3052 - val_acc: 0.9147\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0353 - acc: 0.9990 - val_loss: 0.3039 - val_acc: 0.9109\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0348 - acc: 0.9990 - val_loss: 0.3025 - val_acc: 0.9147\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0331 - acc: 0.9990 - val_loss: 0.3033 - val_acc: 0.9134\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0321 - acc: 1.0000 - val_loss: 0.3045 - val_acc: 0.9122\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0317 - acc: 1.0000 - val_loss: 0.3185 - val_acc: 0.9097\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0332 - acc: 1.0000 - val_loss: 0.3030 - val_acc: 0.9159\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0317 - acc: 0.9990 - val_loss: 0.3111 - val_acc: 0.9097\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0296 - acc: 1.0000 - val_loss: 0.3095 - val_acc: 0.9034\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0293 - acc: 1.0000 - val_loss: 0.3197 - val_acc: 0.9122\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0283 - acc: 1.0000 - val_loss: 0.2986 - val_acc: 0.9159\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 0.0278 - acc: 1.0000 - val_loss: 0.3199 - val_acc: 0.9109\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0273 - acc: 0.9990 - val_loss: 0.2984 - val_acc: 0.9122\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0268 - acc: 1.0000 - val_loss: 0.3123 - val_acc: 0.9122\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0263 - acc: 1.0000 - val_loss: 0.3027 - val_acc: 0.9134\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_test,y_test),\n",
    "                    batch_size=100, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Évaluation des performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les prédictions du réseau de neurones sont évaluées pour tous les exemples de l'échantillon de test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.1344309e-05 7.1412884e-05 1.0588833e-05 4.5387871e-05 6.5384602e-06\n",
      " 1.2550692e-01 5.3597946e-06 7.5986383e-05 1.1356127e-04 8.0707548e-05]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print predictions[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque exemple, la prédiction est un tableau de 10 valeurs. Chaque valeur est la probabilité, estimée par le réseau, que l'image appartienne à une catégorie donnée. \n",
    "\n",
    "La catégorie prédite est celle avec la probabilité la plus grande.  \n",
    "\n",
    "Écrivons maintenant une petite fonction pour afficher une image donnée, et imprimer la catégorie prédite ainsi que la catégorie réelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(index):\n",
    "    print 'predicted probabilities:'\n",
    "    print predictions[index]\n",
    "    print 'predicted category', np.argmax(predictions[index])\n",
    "    print 'true probabilities:'\n",
    "    print y_test[index]\n",
    "    print 'true category', np.argmax(y_test[index])\n",
    "    img = x_test[index].reshape(8,8)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette fonction, on obtient la catégorie avec `np.argmax` qui, pour un tableau, retourne l'index correspondant à la valeur maximum. \n",
    "\n",
    "Utilisons cette fonction pour regarder quelques exemples (changez juste l'index pour choisir un autre exemple):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted probabilities:\n",
      "[4.1344309e-05 7.1412884e-05 1.0588833e-05 4.5387871e-05 6.5384602e-06\n",
      " 1.2550692e-01 5.3597946e-06 7.5986383e-05 1.1356127e-04 8.0707548e-05]\n",
      "predicted category 5\n",
      "true probabilities:\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "true category 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, calculons la précision, c'est à dire la probabilité de classer les chiffres correctement. \n",
    "\n",
    "On calcule cette précision sur l'échantillon de test, qui n'a pas été utilisé dans l'entraînement du réseau. À nouveau, on utilise `np.argmax` pour obtenir les catégories vraies et prédites pour chaque exemple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(797,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9134253450439147"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Le deuxième argument de argmax spécifie\n",
    "# que l'on souhaite conserver la première \n",
    "# dimension du tableau. \n",
    "# Ainsi, argmax est calculé pour chaque \n",
    "# exemple. \n",
    "# Sans cet argument, argmax retournerait\n",
    "# une seule valeur, la probabilité maximum \n",
    "# dans le tableau complet, \n",
    "# en considérant l'ensemble des exemples \n",
    "y_test_best = np.argmax(y_test,1)\n",
    "print y_test_best.shape\n",
    "predictions_best = np.argmax(predictions,1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_best, predictions_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devriez obtenir une précision autour de 91%, similaire à celle que nous avons obtenue dans les mêmes conditions avec scikit-learn. \n",
    "\n",
    "Le résultat n'est pas reproductible, et la précision variera à chaque fois que vous ré-entraînerez un nouveau réseau. Personnellement, j'obtiens généralement une précision entre 90 et 93%.  \n",
    "\n",
    "Et pour vous, que se passe-t'il lorsque vous répétez l'exercice? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Et ensuite? \n",
    "\n",
    "Dans ce post, vous avez entraîné votre premier réseau de neurones avec keras.  \n",
    "\n",
    "Keras est un outil incontournable, et nous l'utiliserons régulièrement sur ce blog. \n",
    "\n",
    "Très bientôt, nous nous lancerons dans le deep learning sur GPU avec keras et TensorFlow. \n",
    "\n",
    "Pour en savoir plus: [Guide to the sequential model of keras](https://keras.io/getting-started/sequential-model-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
