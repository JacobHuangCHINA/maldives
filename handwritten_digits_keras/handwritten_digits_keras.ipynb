{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this tutorial \n",
    "\n",
    "In my post about [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/), we have seen that [scikit-learn](https://scikit-learn.org) makes it easy to train a simple neural network. \n",
    "\n",
    "scikit-learn focuses on providing a unified interface for training and testing many machine learning algorithms: neural networks, support-vector machines, naive Bayes, nearest neighbours, decision trees, etc. Indeed, in machine learning, a large amount of time is spent choosing the best algorithm and tuning it for best performance. Scikit-learn was designed to make these operations as easy as possible.  \n",
    "\n",
    "However, when it comes to neural networks specifically, scikit-learn has three major drawbacks:\n",
    "\n",
    "* the interface does not provide enough control to build complex neural networks\n",
    "* a lot of magic happens under the hood, like regularization\n",
    "* it's not adapted to [deep learning](https://thedatafrog.com/install-tensorflow-windows/)\n",
    "\n",
    "In this post, we will repeat our exercise about handwritten digit recognition with [Keras](https://keras.io/), a high-level neural networks API. You will learn how to: \n",
    "\n",
    "* Create a simple dense neural network with Keras\n",
    "* Estimate its performance \n",
    "\n",
    "Later on, we will use Keras as an interface to TensorFlow to do deep learning. \n",
    "\n",
    "**Prerequisites:** You should have some knowledge of:\n",
    "\n",
    "* numpy, \n",
    "* matplotlib, \n",
    "* jupyter notebooks,\n",
    "* basic neural networks. \n",
    "\n",
    "If that's not yet the case, you can follow [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install [Anaconda](https://www.anaconda.com/download/) if not already done. Please be sure to **pick the version for python 2.X and not for python 3.X**.\n",
    "\n",
    "Launch the Anaconda Navigator application. \n",
    "\n",
    "Enter the Environments tab, and install the keras package in your base (root) environment. \n",
    "\n",
    "Solving the package specifications for keras may take a few minutes but you'll eventually get there.\n",
    "\n",
    "Then, head to the Home tab, and launch the jupyter notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get this notebook and open it in the jupyter notebook:\n",
    "\n",
    "* [download the repository containing this notebook](https://github.com/cbernet/maldives/archive/master.zip)\n",
    "* unzip it, say to `Downloads/maldives-master`\n",
    "* in the jupyter notebook, navigate to `Downloads/maldives-master/handwritten_digits_keras`\n",
    "* open `handwritten_digits_keras.ipynb`\n",
    "\n",
    "This page should appear in the notebook. From now on, follow the tutorial in the notebook. You should execute the cells as they come, or execute them all in one go. You can even add cells or modify existing cells to experiment a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset \n",
    "\n",
    "As in [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/), we are going to use the digits dataset provided by scikit-learn. The digits are 8x8 images that are fed into a neural network with:\n",
    "\n",
    "* an input layer with 8x8 = 64 neurons\n",
    "* a hidden layer with 15 neurons\n",
    "* an output layer with 10 neurons corresponding to the 10 digit categories. \n",
    "\n",
    "First, let's initialize our tools and load the digits dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "# for some reason, the following is needed to run on mac os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'  \n",
    "\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer requires a 1-dimensional array in input, but our images are 2D. So we need to flatten all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels require a bit of attention. At the moment, `digits.target` contains the digit corresponding to each image in the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in Keras, we have to build our neural network with 10 output neurons (this actually happens under the hood in scikit-learn). During the training, Keras will have to compare the 10 output values of these neurons to the target value. But how can we compare a vector of 10 values with a single target value? \n",
    "\n",
    "The solution is to translate each target value into a vector of length 10 with a technique called *one-hot encoding*: \n",
    "\n",
    "* target `0` is translated to `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
    "* target `1` is translated to `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
    "* ...\n",
    "* target `9` is translated to `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`\n",
    "\n",
    "After doing that, the values from the output neurons, which are probabilities ranging from 0 to 1, can be compared directly to the values in the target vector. In this way, for a given number, say 0, the neural network will be trained to output a high probability from the first output neuron, and a low probability from the following neurons.  \n",
    "\n",
    "One-hot encoding can be performed easily with the utilities provided by Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(digits.target,10)\n",
    "print digits.target\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now split our data into a training sample and a testing sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_limit=1000\n",
    "x_train = x[:split_limit]\n",
    "y_train = y[:split_limit]\n",
    "x_test = x[split_limit:]\n",
    "y_test = y[split_limit:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 1000 images and labels are going to be used for training. The rest of the dataset will be used later to test the performance of our network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the neural network. We use one hidden layers with 15 neurons. The output layer must have exactly 10 neurons because we want to classify our digits in 10 categories. Don't pay attention to the other parameters, we'll cover that in future posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Model, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 15)                975       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                160       \n",
      "=================================================================\n",
      "Total params: 1,135\n",
      "Trainable params: 1,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the input layer: \n",
    "img_input = layers.Input(shape=(64,))\n",
    "\n",
    "# create the hidden layer\n",
    "tmp = layers.Dense(15, activation='sigmoid')(img_input)\n",
    "\n",
    "# create the output layer\n",
    "output = layers.Dense(10, activation='sigmoid')(tmp)\n",
    "\n",
    "# create the neural network from the layers\n",
    "model = Model(img_input, output)\n",
    "\n",
    "# print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# =================================================\n",
    "# Please don't pay attention to what follows, \n",
    "# we'll talk about regularization later!\n",
    "l2_rate = 1e-4\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer = regularizers.l2(l2_rate)\n",
    "        layer.bias_regularizer = regularizers.l2(l2_rate)\n",
    "        layer.activity_regularizer = regularizers.l2(l2_rate)\n",
    "# =================================================\n",
    "\n",
    "# define how the neural network will learn, \n",
    "# and compile the model:\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.1, momentum=0.9),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 797 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s 435us/step - loss: 2.2250 - acc: 0.2060 - val_loss: 2.1394 - val_acc: 0.2735\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 2.0153 - acc: 0.4210 - val_loss: 1.9185 - val_acc: 0.5358\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 1.7790 - acc: 0.6280 - val_loss: 1.6579 - val_acc: 0.6989\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.4695 - acc: 0.7260 - val_loss: 1.3503 - val_acc: 0.6813\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 1.1476 - acc: 0.7720 - val_loss: 1.0630 - val_acc: 0.8118\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.8823 - acc: 0.8590 - val_loss: 0.8882 - val_acc: 0.8306\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.6574 - acc: 0.9240 - val_loss: 0.7306 - val_acc: 0.8720\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.5126 - acc: 0.9400 - val_loss: 0.6154 - val_acc: 0.8846\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.4035 - acc: 0.9600 - val_loss: 0.4975 - val_acc: 0.9084\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.3332 - acc: 0.9650 - val_loss: 0.4768 - val_acc: 0.9046\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.2880 - acc: 0.9700 - val_loss: 0.3879 - val_acc: 0.9184\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.2450 - acc: 0.9740 - val_loss: 0.3755 - val_acc: 0.9122\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.2101 - acc: 0.9820 - val_loss: 0.3762 - val_acc: 0.9147\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.2015 - acc: 0.9790 - val_loss: 0.3734 - val_acc: 0.9021\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.1919 - acc: 0.9750 - val_loss: 0.3813 - val_acc: 0.9097\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1853 - acc: 0.9770 - val_loss: 0.3965 - val_acc: 0.8934\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1784 - acc: 0.9750 - val_loss: 0.3947 - val_acc: 0.8934\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.1601 - acc: 0.9780 - val_loss: 0.3575 - val_acc: 0.9072\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.1370 - acc: 0.9860 - val_loss: 0.3433 - val_acc: 0.9034\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1265 - acc: 0.9850 - val_loss: 0.3875 - val_acc: 0.8946\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1575 - acc: 0.9750 - val_loss: 0.3454 - val_acc: 0.9109\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1673 - acc: 0.9680 - val_loss: 0.3844 - val_acc: 0.8733\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 0.1487 - acc: 0.9740 - val_loss: 0.3455 - val_acc: 0.9097\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1209 - acc: 0.9810 - val_loss: 0.3015 - val_acc: 0.9134\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.1160 - acc: 0.9840 - val_loss: 0.3277 - val_acc: 0.9059\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.1028 - acc: 0.9910 - val_loss: 0.3101 - val_acc: 0.9184\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0941 - acc: 0.9900 - val_loss: 0.3272 - val_acc: 0.9046\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.0922 - acc: 0.9900 - val_loss: 0.3136 - val_acc: 0.9109\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0873 - acc: 0.9910 - val_loss: 0.3154 - val_acc: 0.9084\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.0766 - acc: 0.9940 - val_loss: 0.3109 - val_acc: 0.9172\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.0806 - acc: 0.9910 - val_loss: 0.3196 - val_acc: 0.9134\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.0736 - acc: 0.9920 - val_loss: 0.3166 - val_acc: 0.9134\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.0778 - acc: 0.9920 - val_loss: 0.3228 - val_acc: 0.9122\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.0715 - acc: 0.9930 - val_loss: 0.3225 - val_acc: 0.9134\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.0629 - acc: 0.9950 - val_loss: 0.2959 - val_acc: 0.9109\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0595 - acc: 0.9950 - val_loss: 0.3274 - val_acc: 0.9134\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0571 - acc: 0.9950 - val_loss: 0.2823 - val_acc: 0.9210\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0521 - acc: 0.9960 - val_loss: 0.2982 - val_acc: 0.9172\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0531 - acc: 0.9960 - val_loss: 0.2989 - val_acc: 0.9210\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0538 - acc: 0.9960 - val_loss: 0.2922 - val_acc: 0.9147\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0517 - acc: 0.9960 - val_loss: 0.3174 - val_acc: 0.9159\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0561 - acc: 0.9960 - val_loss: 0.2922 - val_acc: 0.9235\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0492 - acc: 0.9950 - val_loss: 0.3234 - val_acc: 0.9072\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0475 - acc: 0.9960 - val_loss: 0.2710 - val_acc: 0.9272\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0440 - acc: 0.9960 - val_loss: 0.3029 - val_acc: 0.9197\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0452 - acc: 0.9960 - val_loss: 0.2814 - val_acc: 0.9247\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0426 - acc: 0.9970 - val_loss: 0.2891 - val_acc: 0.9260\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0393 - acc: 0.9970 - val_loss: 0.2815 - val_acc: 0.9285\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0380 - acc: 0.9970 - val_loss: 0.2825 - val_acc: 0.9297\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0366 - acc: 0.9970 - val_loss: 0.2793 - val_acc: 0.9285\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_test,y_test),\n",
    "                    batch_size=100, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions from the neural network are evaluated for all examples in the test sample by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.45888440e-06 1.03058234e-01 4.93021728e-03 2.87596176e-05\n",
      " 7.25093851e-05 4.68176258e-05 7.78189860e-04 5.31724472e-05\n",
      " 6.63962099e-04 7.51014159e-06]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample, the prediction is an array of 10 values. Each value is the estimated probability for the image to belong to this category. \n",
    "\n",
    "The predicted category is the one with the largest probability. \n",
    "\n",
    "Let's write a small function to plot a given image, and to print the true and predicted categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(index):\n",
    "    print 'predicted probabilities:'\n",
    "    print predictions[index]\n",
    "    print 'predicted category', np.argmax(predictions[index])\n",
    "    print 'true probabilities:'\n",
    "    print y_test[index]\n",
    "    print 'true category', np.argmax(y_test[index])\n",
    "    img = x_test[index].reshape(8,8)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we obtain the category with `np.argmax` that, for an array, returns the index corresponding to the largest value. \n",
    "\n",
    "Let's use this function to have a look at a few examples (just choose a different index to look at another example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted probabilities:\n",
      "[6.45888440e-06 1.03058234e-01 4.93021728e-03 2.87596176e-05\n",
      " 7.25093851e-05 4.68176258e-05 7.78189860e-04 5.31724472e-05\n",
      " 6.63962099e-04 7.51014159e-06]\n",
      "predicted category 1\n",
      "true probabilities:\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "true category 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the accuracy score, which is the probability to classify the digits correctly.\n",
    "\n",
    "We will compute the accuracy for the test sample, which has not been used to train the network. Again, we will use `np.argmax` to get the predicted and true categories for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(797,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9284818067754078"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second argument of argmax specifies\n",
    "# that we want to get argmax for each example. \n",
    "# without this argument, argmax would return \n",
    "# the largest value in the whole array,\n",
    "# considering all examples\n",
    "y_test_best = np.argmax(y_test,1)\n",
    "print y_test_best.shape\n",
    "predictions_best = np.argmax(predictions,1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_best, predictions_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should obtain an accuracy around 91%, similar to the one we had obtained in the same conditions with scikit-learn. \n",
    "\n",
    "Please note that the result is not deterministic, so the accuracy will vary. I usually get an accuracy between 90 and 93%. \n",
    "\n",
    "You can try to repeat the exercise starting from the creation of the neural network to see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
