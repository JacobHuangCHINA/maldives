{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this tutorial \n",
    "\n",
    "In my post about [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/), we have seen that a simple neural network can easily be trained with [scikit-learn](https://scikit-learn.org). And we managed to classify images of digits with an accuracy larger than 90%. \n",
    "\n",
    "But scikit-learn is generally not adapted to neural networks. \n",
    "\n",
    "Its goal is in fact to provide a unified interface for training and testing different machine learning algorithms: neural networks, and also support-vector machines, naive Bayes, nearest neighbours, decision trees, etc. \n",
    "\n",
    "Indeed, in machine learning projects, you'll spend a large fraction of your time choosing the best algorithm and tuning it for best performance. Scikit-learn was designed to make these tasks as easy as possible.  \n",
    "\n",
    "However, when it comes to neural networks specifically, scikit-learn has two major drawbacks:\n",
    "\n",
    "* the interface does not provide enough control to build complex neural networks, nor to control their behaviour in details.\n",
    "* it's not adapted to [deep learning](https://thedatafrog.com/install-tensorflow-windows/)\n",
    "\n",
    "In this post, we will repeat our exercise about handwritten digit recognition with [Keras](https://keras.io/), a high-level neural networks API. You will learn:\n",
    "\n",
    "* How to install Keras \n",
    "* How to create a simple dense neural network with this tool \n",
    "* How to estimate its performance \n",
    "\n",
    "Later on, we will use Keras as an interface to TensorFlow to do deep learning. \n",
    "\n",
    "**Prerequisites:** You should have some knowledge of:\n",
    "\n",
    "* numpy, \n",
    "* matplotlib, \n",
    "* jupyter notebooks,\n",
    "* basic neural networks. \n",
    "\n",
    "If that's not the case, [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/) is just for you!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install [Anaconda](https://www.anaconda.com/download/) if not already done. Please be sure to **pick the version for python 2.X and not for python 3.X**.\n",
    "\n",
    "Launch the Anaconda Navigator application. \n",
    "\n",
    "Enter the Environments tab, and install the keras package in your base (root) environment. \n",
    "\n",
    "Solving the package specifications for keras may take a few minutes but you'll eventually get there.\n",
    "\n",
    "Then, head to the Home tab, and launch the jupyter notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, get this notebook and open it in the jupyter notebook:\n",
    "\n",
    "* [download the repository containing this notebook](https://github.com/cbernet/maldives/archive/master.zip)\n",
    "* unzip it, say to `Downloads/maldives-master`\n",
    "* in the jupyter notebook, navigate to `Downloads/maldives-master/handwritten_digits_keras`\n",
    "* open `handwritten_digits_keras.ipynb`\n",
    "\n",
    "This page should appear in the notebook. From now on, follow the tutorial in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset \n",
    "\n",
    "As in [handwritten digit recognition with scikit-learn](https://thedatafrog.com/handwritten-digit-recognition-scikit-learn/), we are going to use the digits dataset provided by scikit-learn. The digits are 8x8 images and we will feed them to a neural network with:\n",
    "\n",
    "* an input layer with 8x8 = 64 neurons\n",
    "* a hidden layer with 15 neurons\n",
    "* an output layer with 10 neurons corresponding to the 10 digit categories. \n",
    "\n",
    "First, let's initialize our tools and load the digits dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "# for some reason, the following is needed to run on mac os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'  \n",
    "\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer requires a 1-dimensional array in input, but our images are 2D. So we need to flatten all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels require a bit of attention. At the moment, `digits.target` contains the digit corresponding to each image in the dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in Keras, we have to build our neural network with 10 output neurons (this actually happens under the hood in scikit-learn). During the training, Keras will have to compare the 10 output values of these neurons to the target value. But how can we compare a vector of 10 values with a single target value? \n",
    "\n",
    "The solution is to translate each target value into a vector of length 10 with a technique called *one-hot encoding*: \n",
    "\n",
    "* target `0` is translated to `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
    "* target `1` is translated to `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`\n",
    "* ...\n",
    "* target `9` is translated to `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`\n",
    "\n",
    "After doing that, the values from the output neurons, which are probabilities ranging from 0 to 1, can be compared directly to the values in the target vector. In this way, for a given number, say 0, the neural network will be trained to output a high probability from the first output neuron, and a low probability from the following neurons.  \n",
    "\n",
    "One-hot encoding can be performed easily with the utilities provided by Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(digits.target,10)\n",
    "print digits.target\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now split our data into a training sample and a testing sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_limit=1000\n",
    "x_train = x[:split_limit]\n",
    "y_train = y[:split_limit]\n",
    "x_test = x[split_limit:]\n",
    "y_test = y[split_limit:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 1000 images and labels are going to be used for training. The rest of the dataset will be used later to test the performance of our network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the neural network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the necessary tools from Keras, we create the neural network in the following code snippet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Model, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                975       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                160       \n",
      "=================================================================\n",
      "Total params: 1,135\n",
      "Trainable params: 1,135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the input layer\n",
    "# \n",
    "# we specify that the input layer \n",
    "# should have 64 neurons, one for each pixel\n",
    "# in our images. \n",
    "# The input neurons do nothing, they \n",
    "# just transfer the value at each pixel \n",
    "# to the next layer. \n",
    "img_input = layers.Input(shape=(64,))\n",
    "\n",
    "# create the hidden layer\n",
    "#\n",
    "# This layer is a Dense layer, which means\n",
    "# that its neurons are fully connected to the \n",
    "# neurons in the previous layer (the input layer)\n",
    "# We will talk about the activation in a future post\n",
    "tmp = layers.Dense(15, \n",
    "                   activation='sigmoid')(img_input)\n",
    "\n",
    "# create the output layer\n",
    "# \n",
    "# The output layer is another Dense layer.\n",
    "# It must have 10 neurons, corresponding to \n",
    "# the 10 digit categories \n",
    "output = layers.Dense(10, \n",
    "                      activation='softmax')(tmp)\n",
    "\n",
    "# create the neural network from the layers\n",
    "model = Model(img_input, output)\n",
    "\n",
    "# print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# =================================================\n",
    "# Please don't pay attention to what follows, \n",
    "# we'll talk about regularization later!\n",
    "# For now, it is enough to know that regularization\n",
    "# helps the neural network converge properly. \n",
    "# I've added this regularization because it is \n",
    "# performed by default in scikit-learn, \n",
    "# and because we want to be able to compare the \n",
    "# results of scikit-learn and keras. \n",
    "l2_rate = 1e-4\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer = regularizers.l2(l2_rate)\n",
    "        layer.bias_regularizer = regularizers.l2(l2_rate)\n",
    "        layer.activity_regularizer = regularizers.l2(l2_rate)\n",
    "# =================================================\n",
    "\n",
    "# define how the neural network will learn, \n",
    "# and compile the model. \n",
    "# models must be compiled before \n",
    "# they can be trained and used. \n",
    "# the loss, optimizer, and metrics arguments \n",
    "# will be covered in a future post. \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.1, momentum=0.9),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 797 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s 430us/step - loss: 2.2570 - acc: 0.2060 - val_loss: 1.9799 - val_acc: 0.4391\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.7615 - acc: 0.5700 - val_loss: 1.5124 - val_acc: 0.6173\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 1.2999 - acc: 0.7120 - val_loss: 1.2052 - val_acc: 0.7327\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.9755 - acc: 0.8360 - val_loss: 0.9633 - val_acc: 0.7917\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.7352 - acc: 0.8570 - val_loss: 0.8170 - val_acc: 0.8105\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.5977 - acc: 0.8900 - val_loss: 0.6957 - val_acc: 0.8306\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.4799 - acc: 0.9130 - val_loss: 0.5716 - val_acc: 0.8770\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.4047 - acc: 0.9270 - val_loss: 0.5378 - val_acc: 0.8720\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s 55us/step - loss: 0.3405 - acc: 0.9460 - val_loss: 0.5185 - val_acc: 0.8808\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.2989 - acc: 0.9480 - val_loss: 0.5236 - val_acc: 0.8657\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.2653 - acc: 0.9590 - val_loss: 0.4815 - val_acc: 0.8908\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.2489 - acc: 0.9490 - val_loss: 0.4888 - val_acc: 0.8795\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.2339 - acc: 0.9600 - val_loss: 0.4344 - val_acc: 0.8971\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.2089 - acc: 0.9620 - val_loss: 0.4893 - val_acc: 0.8632\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.2012 - acc: 0.9580 - val_loss: 0.4107 - val_acc: 0.8934\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1801 - acc: 0.9640 - val_loss: 0.4007 - val_acc: 0.8921\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1633 - acc: 0.9720 - val_loss: 0.3830 - val_acc: 0.9021\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s 52us/step - loss: 0.1475 - acc: 0.9710 - val_loss: 0.4084 - val_acc: 0.8946\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.1449 - acc: 0.9720 - val_loss: 0.4290 - val_acc: 0.8770\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s 57us/step - loss: 0.1393 - acc: 0.9720 - val_loss: 0.4002 - val_acc: 0.8959\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.1190 - acc: 0.9790 - val_loss: 0.3784 - val_acc: 0.8934\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.1172 - acc: 0.9770 - val_loss: 0.3580 - val_acc: 0.8971\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.1304 - acc: 0.9740 - val_loss: 0.3794 - val_acc: 0.8971\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.1127 - acc: 0.9810 - val_loss: 0.3616 - val_acc: 0.8996\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 0.0884 - acc: 0.9890 - val_loss: 0.3453 - val_acc: 0.9072\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0823 - acc: 0.9880 - val_loss: 0.3525 - val_acc: 0.9021\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0911 - acc: 0.9830 - val_loss: 0.3605 - val_acc: 0.9034\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.1099 - acc: 0.9790 - val_loss: 0.4343 - val_acc: 0.8770\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.1251 - acc: 0.9720 - val_loss: 0.3649 - val_acc: 0.9009\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.1068 - acc: 0.9740 - val_loss: 0.3727 - val_acc: 0.8946\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 0.0902 - acc: 0.9830 - val_loss: 0.3856 - val_acc: 0.8795\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.1281 - acc: 0.9630 - val_loss: 0.3476 - val_acc: 0.9072\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0833 - acc: 0.9880 - val_loss: 0.3495 - val_acc: 0.9109\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0858 - acc: 0.9880 - val_loss: 0.3507 - val_acc: 0.9084\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s 63us/step - loss: 0.0862 - acc: 0.9890 - val_loss: 0.3509 - val_acc: 0.9021\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s 87us/step - loss: 0.0698 - acc: 0.9890 - val_loss: 0.3407 - val_acc: 0.9059\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 0.0628 - acc: 0.9910 - val_loss: 0.3355 - val_acc: 0.9059\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.0602 - acc: 0.9890 - val_loss: 0.3490 - val_acc: 0.9046\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s 49us/step - loss: 0.0612 - acc: 0.9940 - val_loss: 0.3303 - val_acc: 0.9134\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.0664 - acc: 0.9880 - val_loss: 0.3398 - val_acc: 0.9084\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0663 - acc: 0.9890 - val_loss: 0.3624 - val_acc: 0.9021\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0634 - acc: 0.9840 - val_loss: 0.3461 - val_acc: 0.9147\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0532 - acc: 0.9920 - val_loss: 0.3476 - val_acc: 0.9134\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0482 - acc: 0.9950 - val_loss: 0.3428 - val_acc: 0.9159\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0445 - acc: 0.9950 - val_loss: 0.3268 - val_acc: 0.9147\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0435 - acc: 0.9970 - val_loss: 0.3308 - val_acc: 0.9059\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0405 - acc: 0.9970 - val_loss: 0.3252 - val_acc: 0.9172\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0426 - acc: 0.9950 - val_loss: 0.3404 - val_acc: 0.9122\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0430 - acc: 0.9940 - val_loss: 0.3306 - val_acc: 0.9122\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0421 - acc: 0.9960 - val_loss: 0.3342 - val_acc: 0.9109\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train, y=y_train, validation_data=(x_test,y_test),\n",
    "                    batch_size=100, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions from the neural network are evaluated for all examples in the test sample by doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.22454715e-05 1.40154385e-04 1.14361326e-04 1.87753886e-03\n",
      " 5.77343690e-06 9.97086465e-01 3.70808011e-05 4.92060144e-06\n",
      " 3.18983046e-04 3.42447049e-04]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print predictions[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample, the prediction is an array of 10 values. Each value is the estimated probability for the image to belong to this category. \n",
    "\n",
    "The predicted category is the one with the largest probability. \n",
    "\n",
    "Let's write a small function to plot a given image, and to print the true and predicted categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(index):\n",
    "    print 'predicted probabilities:'\n",
    "    print predictions[index]\n",
    "    print 'predicted category', np.argmax(predictions[index])\n",
    "    print 'true probabilities:'\n",
    "    print y_test[index]\n",
    "    print 'true category', np.argmax(y_test[index])\n",
    "    img = x_test[index].reshape(8,8)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, we obtain the category with `np.argmax` that, for an array, returns the index corresponding to the largest value. \n",
    "\n",
    "Let's use this function to have a look at a few examples (just choose a different index to look at another example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted probabilities:\n",
      "[7.22454715e-05 1.40154385e-04 1.14361326e-04 1.87753886e-03\n",
      " 5.77343690e-06 9.97086465e-01 3.70808011e-05 4.92060144e-06\n",
      " 3.18983046e-04 3.42447049e-04]\n",
      "predicted category 5\n",
      "true probabilities:\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "true category 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the accuracy score, which is the probability to classify the digits correctly.\n",
    "\n",
    "We will compute the accuracy for the test sample, which has not been used to train the network. Again, we will use `np.argmax` to get the predicted and true categories for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(797,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9109159347553325"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second argument of argmax specifies\n",
    "# that we want to get argmax for each example. \n",
    "# without this argument, argmax would return \n",
    "# the largest value in the whole array,\n",
    "# considering all examples\n",
    "y_test_best = np.argmax(y_test,1)\n",
    "print y_test_best.shape\n",
    "predictions_best = np.argmax(predictions,1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_best, predictions_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should obtain an accuracy around 91%, similar to the one we had obtained in the same conditions with scikit-learn. \n",
    "\n",
    "Please note that the result is not deterministic, so the accuracy will vary every time you train the network. I usually get an accuracy between 90 and 93%, but I sometimes get a value as low as 87%. \n",
    "\n",
    "Please repeat the exercise starting from the creation of the neural network to see what happens. \n",
    "\n",
    "## What next? \n",
    "\n",
    "In this post, you have trained your first neural network with keras. \n",
    "\n",
    "Keras is the easiest and most powerful way to work with neural networks, and we will use it very often on this blog. \n",
    "\n",
    "Very soon, we'll start to do deep learning on a GPU with keras and TensorFlow. \n",
    "\n",
    "More information: [Guide to the sequential model of keras](https://keras.io/getting-started/sequential-model-guide/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
