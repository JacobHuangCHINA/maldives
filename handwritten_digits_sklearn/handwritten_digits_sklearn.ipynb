{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why machine learning? \n",
    "\n",
    "* give a few applications \n",
    "* explain why machine learning is superior to procedural programming\n",
    "* convince programmers they must learn it, and companies they must board the train now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning for classification\n",
    "\n",
    "Let's get a basic feel for how a neural network can be trained for classification. \n",
    "\n",
    "In this post, our goal is to get started hands on with machine learning fast and easy, so this explanation of machine learning is over simplified. But there will definitely be a more detailed post about the training principles later on, so stay tuned if you're interested. \n",
    "\n",
    "![Supervised learning](https://github.com/cbernet/maldives/raw/master/handwritten_digits_sklearn/supervised_learning.png)\n",
    "\n",
    "The network is presented with a succession of training examples. Each training example consists of: \n",
    "\n",
    "* the image of a digit\n",
    "* a label, which tells us which digit the image truly represents. For a given image, the label could be told to us by the person who wrote the digit in the first place. \n",
    "\n",
    "In the drawing above, the first image is processed by the neural network, which produces an answer: this is a 9.  \n",
    "At first, the connections between the neurons in the network are random, and the network is not able to do anything useful. It just provides a random answer. \n",
    "\n",
    "The answer is compared to the label. In this case, the answer (9) is different from the label (the digit is actually a 3), and some feedback is given to the neural network so that it can improve. The connections between the neurons are modified, favouring the ones that tend to give a correct answer. \n",
    "\n",
    "After the modification, the next examples are considered, and the neural network learns in an iterative process. \n",
    "\n",
    "The number of training examples needed to train the network properly could be of the order of a few hundred for networks with a simple architecture, and millions for complex networks.  \n",
    "\n",
    "## Installing python and its scientific library\n",
    "\n",
    "**if you're already running this tutorial in your jupyter notebook, please skip this section.**\n",
    "\n",
    "We will use a variety of tools from scipy, the scientific python library: \n",
    "\n",
    "* **scikit-learn**: one of leading machine-learning toolkits for python. It will provide an easy access to the handwritten digits dataset, and allow us to define and train our neural network in a few lines of code\n",
    "* **numpy**: core package providing powerful tools to manipulate data arrays, such as our digit images\n",
    "* **matplotlib**: visualization tools, essential to check what we are doing\n",
    "* **jupyter**: the web server that will allow you to follow this tutorial and run the code in your web browser. \n",
    "\n",
    "Scipy is actually not a single library, but an \"ecosystem\" of interdependent python packages.\n",
    "\n",
    "This ecosystem is full of snakes and beasts fighting survival -- you do not want to hang in there alone.  \n",
    "\n",
    "And indeed, six years ago, when I first got started with scipy, I tried to install manually all the packages I needed on top of the version of python already installed in my system. \n",
    "\n",
    "I spent almost a day fighting against conflicting dependencies for these packages. For example, scikit-learn might need numpy version A, but pandas needs numpy version B. Or, one of these packages requires a version of python more recent than the one you have, meaning that you need to install an additional version of python and deal with your two versions later on.   \n",
    "\n",
    "And then, I discovered Anaconda. \n",
    "\n",
    "As stated on Anaconda's website: \n",
    "\n",
    "*With over 6 million users, the open source Anaconda Distribution is the fastest and easiest way to do Python and R data science and machine learning on Linux, Windows, and Mac OS X. It's the industry standard for developing, testing, and training on a single machine.*\n",
    "\n",
    "In a nutshell, the anaconda team maintains a repository of more than 1400 data science packages, all compatible, and provides tools to install a version of python and these packages at the push of a button, and under five minutes. \n",
    "\n",
    "Let's do it now!\n",
    "\n",
    "First, [download anaconda](https://www.anaconda.com/download/) for your system: \n",
    "\n",
    "* Choose the python 2.X version, not the 3.X version.\n",
    "* If you're using windows or linux, make sure to pick the 64bit installer if you have a 64bit system. \n",
    "\n",
    "Run the installer, and finally start the Anaconda Navigator. On windows, you can find it by clicking the windows start button, and typing anaconda. \n",
    "\n",
    "In the Anaconda Navigator window, click on the Home tab, and launch the jupyter notebook. \n",
    "\n",
    "Create a new notebook. In your notebook, you should see an empty cell, where you can write python code. Copy-paste the following lines, and execute the cell by pressing shift + enter.  \n",
    "\n",
    "```python\n",
    "print 'hello world!'\n",
    "for i in range(10):\n",
    "    print i\n",
    "```\n",
    "\n",
    "A new cell appears. Import numpy and matplotlib (remember that you need to execute the cell):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "```\n",
    "\n",
    "This is a standard way to import these modules: \n",
    "\n",
    "* the pyplot module of matplotlib is called plt in this context\n",
    "* the numpy module is called np \n",
    "\n",
    "You can very well choose other names, but these ones are used by almost everybody, so it's easier to use them as well. \n",
    "\n",
    "Now let's try and do our first plot, just to make sure that numpy and matplotlib are working:\n",
    "\n",
    "```python \n",
    "# create a numpy 1-D array with evenly spaced values, from 0 to 10 \n",
    "x = np.linspace(0, 10, 11)\n",
    "print x \n",
    "# create a new numpy array. x**2 means that each element of x is squared\n",
    "y = x**2\n",
    "print y\n",
    "# plot y versus x, you should get a parabola. check that for x = 1 we have y = 1, and that for x = 2, y = 4. \n",
    "plt.plot(x, y)\n",
    "```\n",
    "\n",
    "--- \n",
    "\n",
    "**A word of caution:**\n",
    "\n",
    "It is easy to get lost in the documentation of all these tools, and to waste a lot of time. \n",
    "\n",
    "For example, if you check the documentation of the plt.plot method (I won't give you the link ;-) but you could google it), you will see that there are lots of ways to call it, with many optional parameters. But after all, do we need to know more than this: `plt.plot(x,y)` plots y vs x ? \n",
    "\n",
    "If you want to have fun, I suggest to follow this tutorial until the end without digging deeper. \n",
    "\n",
    "You'll train your first neural net easily and in the process, you'll get an understanding of the most important scikit-learn, numpy, and matplotlib tools. That's more than enough for a variety of machine learning tasks, and you can always learn more about specific features of these tools when you need them later on (you'll know!) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have access to the jupyter notebook, I have good news. You won't need to keep copy-pasting code from this page to your notebook. Instead, just download [this notebook](https://github.com/cbernet/maldives/blob/master/handwritten_digits_sklearn/handwritten_digits_sklearn.ipynb) and open it with the jupyter notebook. \n",
    "\n",
    "**FIND A WAY TO PROVIDE DIRECT DOWNLOAD ACCESS TO THE NOTEBOOK. THINK ABOUT ADDITIONAL FILES LIKE IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST digits dataset\n",
    "\n",
    "scikit-learn comes with several test datasets. Let's load the MNIST handwritten digits dataset, and let's print some information about this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, the `dir` function returns the names of the attributes of an object. Let's use this function to check what can be found in the digits object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look in more details at the other attributes. We are going to start by checking the type of each attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for attr in dir(digits):\n",
    "    print type(getattr(digits, attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data, images, target, and target names are all ndarrays (N-dimensional arrays) from the numpy package. The shape attribute of an ndarray gives the number of dimensions and the size along each dimension of the array. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "digits.image is an array with 3 dimensions. The first dimension indexes images, and we see that we have 1797 images in total. The next two dimensions correspond to the x and y coordinates of the pixels in each image. Each image has 8x8 = 64 pixels. In other words, this array could be represented in 3D as a pile of images with 8x8 pixels each. Let's plot the first image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at the data of the first 8x8 image. Each slot in the array corresponds to a pixel, and the value in the slot is the amount of black in the pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print digits.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a20560ad0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digits.images[0],cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is low resolution. The original digits were of much higher resolution, and the resolution has been decreased when creating the dataset to make it easier to train a machine learning algorithm to recognize these digits. \n",
    "\n",
    "Now let's investigate the target attribute. It is a 1-dimensional array with 1797 slots. Looking into the array, we see that it contains the true numbers corresponding to each image. For example, the first target is 0, and corresponds to the image drawn just above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n",
      "[0 1 2 ... 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "print digits.target.shape\n",
    "print digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some more images using this function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(i):\n",
    "    '''Plots 16 digits, starting with digit i'''\n",
    "    nplots = 16\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    for j in range(nplots):\n",
    "        plt.subplot(4,4,j+1)\n",
    "        plt.imshow(digits.images[i+j], cmap='binary')\n",
    "        plt.title(digits.target[i+j])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multi(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can have a look at the next digits by calling `plot_multi(16)`, `plot_multi(32)`, etc. You will probably see that with such a low resolution, it's quite difficult to recognize some of the digits, even for a human. In these conditions, our neural network will also be limited by the low quality of the input images. Can the neural network perform at least as well as a human? It would already be a good achievement! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network and preparing the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With [scikit-learn](https://scikit-learn.org), creating, training, and evaluating a neural network can be done with only a few lines of code. \n",
    "\n",
    "We will make a very simple neural network, with three layers: \n",
    "\n",
    "* an input layer, which will map to the pixels in the input image\n",
    "* a hidden layer\n",
    "* an output layer with 10 neurons corresponding to our 10 classes of digits, from 0 to 9. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multilayer perceptron requires a 1-dimensional array in input, but our images are 2-d. So we need to flatten all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = digits.images, digits.target\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 1797 flattened images. The two dimensions of our 8x8 images have been collapsed into a single dimension by  writing the rows of 8 pixels as they come, one after the other. The first image that we looked at earlier is now represented by a 1-D array with 8x8 = 64 slots. Please check that the values are the same as in the original 2-D image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now split our data into a training sample and a testing sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:1000]\n",
    "y_train = y[:1000]\n",
    "x_test = x[1000:]\n",
    "y_test = y[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our multilayer perceptron. As a first try, we use one hidden layers with 50 neurons. The output layer must have exactly 10 neurons because we want to classify our digits in 10 categories. Don't pay attention to the other parameters, we'll cover that in future posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), activation='logistic', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1, verbose=True)\n",
    "mlp.classes_ = range(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train the MLP: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15019610\n",
      "Iteration 2, loss = 1.49780507\n",
      "Iteration 3, loss = 0.91623697\n",
      "Iteration 4, loss = 0.54269819\n",
      "Iteration 5, loss = 0.33536513\n",
      "Iteration 6, loss = 0.22530172\n",
      "Iteration 7, loss = 0.16567440\n",
      "Iteration 8, loss = 0.12843622\n",
      "Iteration 9, loss = 0.10363953\n",
      "Iteration 10, loss = 0.08751211\n",
      "Iteration 11, loss = 0.07594162\n",
      "Iteration 12, loss = 0.06681904\n",
      "Iteration 13, loss = 0.06052683\n",
      "Iteration 14, loss = 0.05464128\n",
      "Iteration 15, loss = 0.04954497\n",
      "Iteration 16, loss = 0.04541317\n",
      "Iteration 17, loss = 0.04266085\n",
      "Iteration 18, loss = 0.03966078\n",
      "Iteration 19, loss = 0.03651245\n",
      "Iteration 20, loss = 0.03428558\n",
      "Iteration 21, loss = 0.03250458\n",
      "Iteration 22, loss = 0.03085384\n",
      "Iteration 23, loss = 0.02933832\n",
      "Iteration 24, loss = 0.02790287\n",
      "Iteration 25, loss = 0.02672239\n",
      "Iteration 26, loss = 0.02536589\n",
      "Iteration 27, loss = 0.02435473\n",
      "Iteration 28, loss = 0.02329219\n",
      "Iteration 29, loss = 0.02250241\n",
      "Iteration 30, loss = 0.02158760\n",
      "Iteration 31, loss = 0.02088018\n",
      "Iteration 32, loss = 0.02028292\n",
      "Iteration 33, loss = 0.01950331\n",
      "Iteration 34, loss = 0.01897503\n",
      "Iteration 35, loss = 0.01829080\n",
      "Iteration 36, loss = 0.01774998\n",
      "Iteration 37, loss = 0.01722708\n",
      "Iteration 38, loss = 0.01676523\n",
      "Iteration 39, loss = 0.01634158\n",
      "Iteration 40, loss = 0.01593722\n",
      "Iteration 41, loss = 0.01545375\n",
      "Iteration 42, loss = 0.01508451\n",
      "Iteration 43, loss = 0.01477830\n",
      "Iteration 44, loss = 0.01434790\n",
      "Iteration 45, loss = 0.01400548\n",
      "Iteration 46, loss = 0.01366573\n",
      "Iteration 47, loss = 0.01336029\n",
      "Iteration 48, loss = 0.01308567\n",
      "Iteration 49, loss = 0.01281038\n",
      "Iteration 50, loss = 0.01251213\n",
      "Iteration 51, loss = 0.01225689\n",
      "Iteration 52, loss = 0.01202458\n",
      "Iteration 53, loss = 0.01175606\n",
      "Iteration 54, loss = 0.01157799\n",
      "Iteration 55, loss = 0.01131586\n",
      "Iteration 56, loss = 0.01108260\n",
      "Iteration 57, loss = 0.01089283\n",
      "Iteration 58, loss = 0.01065500\n",
      "Iteration 59, loss = 0.01047670\n",
      "Iteration 60, loss = 0.01029393\n",
      "Iteration 61, loss = 0.01011194\n",
      "Iteration 62, loss = 0.00990699\n",
      "Iteration 63, loss = 0.00973053\n",
      "Iteration 64, loss = 0.00957435\n",
      "Iteration 65, loss = 0.00941280\n",
      "Iteration 66, loss = 0.00926523\n",
      "Iteration 67, loss = 0.00909537\n",
      "Iteration 68, loss = 0.00897125\n",
      "Iteration 69, loss = 0.00881597\n",
      "Iteration 70, loss = 0.00866834\n",
      "Iteration 71, loss = 0.00856316\n",
      "Iteration 72, loss = 0.00843640\n",
      "Iteration 73, loss = 0.00827574\n",
      "Iteration 74, loss = 0.00818978\n",
      "Iteration 75, loss = 0.00803294\n",
      "Iteration 76, loss = 0.00793408\n",
      "Iteration 77, loss = 0.00781494\n",
      "Iteration 78, loss = 0.00770416\n",
      "Iteration 79, loss = 0.00760672\n",
      "Iteration 80, loss = 0.00750419\n",
      "Iteration 81, loss = 0.00739368\n",
      "Iteration 82, loss = 0.00729570\n",
      "Iteration 83, loss = 0.00720421\n",
      "Iteration 84, loss = 0.00711990\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(50,), learning_rate='constant',\n",
       "       learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the MLP has been trained, let's see what it can say about our test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 9, 7, 9, 5,\n",
       "       4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4,\n",
       "       5, 6, 7, 8, 9, 0])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp.predict(x_test)\n",
    "predictions[:50] # we just look at the 1st 50 examples in the training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions should be fairly close to the targets of our training sample. Let's check by eye (please compare the values of these arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5,\n",
       "       4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4,\n",
       "       5, 6, 7, 8, 9, 0])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:50] # true labels for the 1st 50 examples in the training sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! we see that most (if not all) predictions match the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But can we be a bit more quantitative? We can compute the accuracy of the classifier, which the probability for a digit to be classified in the right category. Again, scikit-learn comes with a handy tool to do that: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9410288582183187"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We managed to get a 94% accuracy with this very simple neural network!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In future posts, we will:\n",
    "\n",
    "* optimize our network to further increase the accuracy,\n",
    "* use deep learning to reach extreme accuracies,\n",
    "* dive a bit more into the mechanism of the training to understand why we have created the MLP with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
