{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"numba_cuda_kernel.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fm8wdYqVBeXj","colab_type":"text"},"source":["## Numba + CUDA on Google Colab\n","\n","By default, Google Colab is not able to run numba + CUDA, because two lilbraries are not found, `libdevice` and `libnvvm.so`. So we need to make sure that these libraries are found in the notebook.  \n","\n","First, we look for these libraries on the system. To do that, we simply run the `find` command, to recursively look for these libraries starting at the root of the filesystem. The exclamation mark escapes the line so that it's executed by the Linux shell, and not by the jupyter notebook. "]},{"cell_type":"code","metadata":{"id":"5Mt7dgLwmnVJ","colab_type":"code","outputId":"962fa7eb-245b-4c14-c1af-e824fc1558b4","executionInfo":{"status":"ok","timestamp":1565686355173,"user_tz":-120,"elapsed":7168,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!find / -iname 'libdevice'\n","!find / -iname 'libnvvm.so'\n"],"execution_count":70,"outputs":[{"output_type":"stream","text":["/usr/local/cuda-10.0/nvvm/libdevice\n","/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HO0mdhPoDRTS","colab_type":"text"},"source":["Then, we add the two libraries to numba environment variables:"]},{"cell_type":"code","metadata":{"id":"Ctr6aM3cmkdx","colab_type":"code","colab":{}},"source":["import os\n","os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-10.0/nvvm/libdevice\"\n","os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"whscBGwEDcEZ","colab_type":"text"},"source":["And we're done! Now let's get started. "]},{"cell_type":"markdown","metadata":{"id":"iah5fiDRtVYt","colab_type":"text"},"source":["## A first CUDA kernel\n","\n","Let's get started by implementing a very simple CUDA kernel to compute the square root of each value in an array. First, here is our array: "]},{"cell_type":"code","metadata":{"id":"RGzqgz7DuK9n","colab_type":"code","outputId":"2d197cfc-a811-44a6-ce1c-c66d37868c9c","executionInfo":{"status":"ok","timestamp":1565686355173,"user_tz":-120,"elapsed":4622,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import numpy as np\n","a = np.arange(4096,dtype=np.float32)\n","a"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.000e+00, 1.000e+00, 2.000e+00, ..., 4.093e+03, 4.094e+03,\n","       4.095e+03], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"DroQCdZpuU5E","colab_type":"text"},"source":["As we have seen in [part II](https://thedatafrog.com/boost-python-gpu/), and as discussed in the introduction, we can simply use numba's vectorize decorator to compute the square root of all elements in parallel on the GPU: "]},{"cell_type":"code","metadata":{"id":"Dhmy-LmjuRCX","colab_type":"code","outputId":"b08afe7b-c984-43bc-a750-b6e49d26a29f","executionInfo":{"status":"ok","timestamp":1565686355337,"user_tz":-120,"elapsed":2516,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import math\n","from numba import vectorize\n","\n","@vectorize(['float32(float32)'], target='cuda')\n","def gpu_sqrt(x):\n","    return math.sqrt(x)\n","  \n","gpu_sqrt(a)"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n","       63.992188 ], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"i1LUuMmBu2sB","colab_type":"text"},"source":["This time, as an exercise, we'll do the same with a custom CUDA kernel. \n","\n","We first define our kernel: "]},{"cell_type":"code","metadata":{"id":"ln2DfOpjuvfp","colab_type":"code","colab":{}},"source":["from numba import cuda\n","\n","@cuda.jit\n","def gpu_sqrt_kernel(x, out):\n","  idx = cuda.grid(1)\n","  out[idx] = math.sqrt(x[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ruRzBxXgwZyh","colab_type":"text"},"source":["Let's discuss this code in some details. \n","\n","We have an input array of 4096 values, so we will use 4096 threads on the GPU. \n","\n","Our input and output arrays are one dimensional, so we will use a one-dimensional *grid* of threads (we will discuss grids in details in the next section). The call `cuda.grid(1)` returns the unique index for the current thread in the whole grid.  With 4096 threads, `idx` will range from 0 to 4095. \n","\n","Then, we see in the code that each thread is going to deal with a single element of the input array to produce a single element in the output array. This element is determined for each thread by the thread index. \n","\n","Now that we have our kernel, we copy our input array to the GPU device, create an output array on the device with the same shape, and finally launch the kernel: "]},{"cell_type":"code","metadata":{"id":"cN1flx7Dx_nT","colab_type":"code","outputId":"2651670d-3a4a-4706-f7c0-4dc2df50e9ee","executionInfo":{"status":"ok","timestamp":1565676562143,"user_tz":-120,"elapsed":400,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# move input data to the device\n","d_a = cuda.to_device(a)\n","# create output data on the device\n","d_out = cuda.device_array_like(d_a)\n","\n","# we decide to use 32 blocks, each containing 128 threads\n","blocks_per_grid = 32\n","threads_per_block = 128\n","gpu_sqrt_kernel[blocks_per_grid, threads_per_block](d_a, d_out)\n","# wait for all threads to complete\n","cuda.synchronize()\n","# copy the output array back to the host system\n","# and print it\n","print(d_out.copy_to_host())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[ 0.         1.         1.4142135 ... 63.97656   63.98437   63.992188 ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TL3-d3S0z_RM","colab_type":"text"},"source":["In the code above, the 4096 threads are arranged into a grid of 32 *blocks*, each block having 128 threads. This *execution configuration* will be discussed in the next section. \n","\n","**Exercises:**\n","\n","- Go back to the previous cell, and try to decrease the number of blocks per grid, or the number of threads per block. \n","- Then try to increase the number of blocks per grid, or the number of threads per blocks\n","- Try to remove the `cuda.synchronize()` call\n","\n","**Results**: \n","\n","- When you reduce the number of threads, either by decreasing the number of blocks per grid or the number of threads per block, some elements are not processed, and the corresponding slots at the end of the output array remain set to their default value, which is 0. \n","- If, on the other hand, you increase the number of threads, it seems that everything is working fine. However, this actually creates an error even though we cannot see it. We will see later how to expose this error. Debugging code is one of the difficulties of CUDA, as the error messages are not always visible.\n","- Finally, you might have expected that commenting out the call to `cuda.synchronize` would have resulted in output array only partially filled, or not filled at all. That would be a good guess since the CPU keeps running the main program (the one of the cell) while the GPU processes the data asynchronously. However, the copy to the host performs an implicit synchronization, so the call to cuda.syncronize is not necessary.  "]},{"cell_type":"markdown","metadata":{"id":"Ces_Ss67GqzR","colab_type":"text"},"source":["## Execution configuration : number of blocks and number of threads\n","\n","In our first example above, we decided to use 4096 threads. These threads were arranged as a grid containing 32 blocks of 128 threads each. \n","\n","You probably wondered why we decided to do that. So in this section, I will try and answer some of the questions you may have: \n","\n","- Why do we need to arrange threads into blocks within the grid? \n","- How did we come up with the magical numbers 32 and 128? Could we use other values like 16 and 256 (which still total 4096), or 64 and 64? \n","\n","These questions are important if you care about performance -- and you probably do if you're here to learn how to speed up your calculations on a GPU! To answer these questions, we first need to learn about our GPU hardware. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"dAH9lfmuIIRF","colab_type":"text"},"source":["### CUDA: Learn about your GPU hardware\n","\n","Let's find out about the GPU we are using. Please note that your GPU (or the GPU attributed to you on Google Colab) could be different from mine. Of course, the numbers I'm giving below and the picture are only valid for the GPU attributed to me during my session on Google Colab. "]},{"cell_type":"code","metadata":{"id":"fACSmHLzJanZ","colab_type":"code","outputId":"8d32f339-a41f-4dcb-c69c-030f7675eae6","executionInfo":{"status":"ok","timestamp":1565676601128,"user_tz":-120,"elapsed":424,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["cuda.detect()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 1 CUDA devices\n","id 0             b'Tesla T4'                              [SUPPORTED]\n","                      compute capability: 7.5\n","                           pci device id: 4\n","                              pci bus id: 0\n","Summary:\n","\t1/1 devices are supported\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"BM_fu2ZvM5dJ","colab_type":"text"},"source":["We see that's an nvidia [T4](https://www.nvidia.com/en-us/data-center/tesla-t4/), and here is the [white paper with the specs](https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf) for the Turing architecture. In this paper, we see that the T4 is based on the Turing TU102 GPU which has: \n","\n","- 72 *streaming multiprocessors* \n","- 64 cuda cores per streaming multiprocessor\n","\n","Here is a block diagram of the TU102 GPU. The streaming multiprocessors are the 72 inner blocks labelled SM.\n","\n","![TU102 GPU](https://raw.githubusercontent.com/cbernet/maldives/master/numba/tu_102_diagram.jpg)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G75hEplWqIUW","colab_type":"text"},"source":["The [streaming multiprocessors](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) are the fundamental processing units of CUDA enabled GPUs. Each SM can execute hundreds of threads in parallel, and has all the resources needed to manage these threads. "]},{"cell_type":"markdown","metadata":{"id":"0SpW53BJyZgh","colab_type":"text"},"source":["### Number of blocks and number of threads per block\n","\n","When the kernel is launched, each block of threads is sent to a single SM, and each SM can end up with multiple blocks to process. \n","\n","The first conclusion we can draw is that if the number of blocks is lower than the number of SMs in the GPU, some of the SMs will remain idle. That was the case in our first simple example above: we used 32 blocks, and we have 72 SMs on the Tesla T4. Therefore, 40 SMs were not used. \n","\n","To process a block, the SM partitions it into groups of 32 threads called *warps*. A warp executes one common instruction at a given time in parallel for all threads in the warp. So full efficiency is realized if all warps in the block are complete. Therefore, the number of threads per block needs to be a multiple of 32. Moreover, to limit latency, this number should not be too low. \n","\n","Deciding which execution configuration to use is not easy, and the choice should be driven by performance analysis. However, here are some basic rules to get started: \n","\n","- **The number of blocks in the grid should be larger than the number of SMs on the GPU, typically 2 to 4 times larger.** \n","- **The number of threads per block should be a multiple of 32, typically between 128 and 512.**\n","\n","The Tesla T4 has 72 streaming multiprocessors. On paper, it will perform best on data arrays with a size larger than 74 * 2 * 128 ~ 20 000. And this is the bare minimum: this card will typically be used on arrays much larger than that. In our simple example above, we used an array of size 4096, which is way too small.\n","\n","On the other hand, what if the data array is very large? Let's consider an array with one billion entries. Following the second rule above, we can choose a number of threads per block of 256, and we end up with 1e9 / 256 ~ 3 million blocks. Since the CUDA kernel launch overhead increases with the number of blocks, going for such a large number of blocks would seriously hit performance. In the following, we will see how to use striding to solve this problem. "]},{"cell_type":"markdown","metadata":{"id":"rGtoDVrUAFR5","colab_type":"text"},"source":["### Striding \n","\n","This simple kernel deals with a single element of the input array:"]},{"cell_type":"code","metadata":{"id":"PqAPolFrWg8_","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_atan(x, out):\n","  idx = cuda.grid(1)\n","  out[idx] = math.atan(x[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k3F70nyMW1L3","colab_type":"text"},"source":["When the kernel is deployed, the GPU therefore needs to create as many threads as elements in the array, which potentially results in many blocks if the array is large. \n","\n","On the contrary, a striding kernel deals with several elements of the input array, using a loop: "]},{"cell_type":"code","metadata":{"id":"NXP0I0Tk9QNp","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_atan_stride(x, out):\n","  start = cuda.grid(1)\n","  stride = cuda.gridsize(1)\n","  for i in range(start, x.shape[0], stride): \n","    out[i] = math.atan(x[i])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bnIMk8A8Xw_R","colab_type":"text"},"source":["In this way, a given thread deals with several elements, and the number of threads is kept under control. Threads keep doing work in a coordinated way, and the GPU is not wasting time creating and scheduling threads. \n","\n","To understand the code above, let's consider a small example with: \n","\n","- an input data array of size 8\n","- blocks with 4 threads each\n","\n","Without striding, we end up with two blocks, and 8 threads in total, each dealing with a single element in the array. \n","\n","With striding, we get one block, and 4 threads in total. Each thread deals with two elements.  Specifically, in the code above: \n","\n","- `start` is the thread index, which is 0, 1, 2, 3 for the four threads, respectively. \n","- `stride` is the size of the grid, which is the total number of threads in the grid, here 4. \n","- `x.shape[0]` is the size of the input data array `x`, which is 1D. So it's 8. \n","\n","So here are the elements processed by each thread: "]},{"cell_type":"code","metadata":{"id":"I6dK_HEvaJB3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"c101f9d1-6fcd-4345-f719-74e520952073","executionInfo":{"status":"ok","timestamp":1565690598521,"user_tz":-120,"elapsed":410,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["for thread_index in range(4):\n","  print('thread', thread_index)\n","  for j in range(thread_index, 8, 4):\n","    print('\\telement', j)"],"execution_count":79,"outputs":[{"output_type":"stream","text":["thread 0\n","\telement 0\n","\telement 4\n","thread 1\n","\telement 1\n","\telement 5\n","thread 2\n","\telement 2\n","\telement 6\n","thread 3\n","\telement 3\n","\telement 7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BIgZRnm6Jku2","colab_type":"text"},"source":["### Striding on large datasets \n","\n"]},{"cell_type":"code","metadata":{"id":"Ilce8lN07ZEl","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_atan_kernel(x, out):\n","  idx = cuda.grid(1)\n","  out[idx] = math.atan(x[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fPetfTF5NiY","colab_type":"code","colab":{}},"source":["import numpy as np\n","a = np.arange(100000,dtype=np.float32)\n","# move input data to the device\n","d_a = cuda.to_device(a)\n","# create output data on the device\n","d_out = cuda.device_array_like(d_a)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7Ovofmo51EC","colab_type":"text"},"source":["First, let's see how fast we can process this array sequentially (in a single thread) on the GPU:"]},{"cell_type":"code","metadata":{"id":"YM2Q8ys3LKaI","colab_type":"code","outputId":"0082eaa7-d948-4fda-aec7-8b167d050124","executionInfo":{"status":"ok","timestamp":1565682391190,"user_tz":-120,"elapsed":402,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%time gpu_atan_kernel[1, 1](d_a, d_out); cuda.synchronize()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 0 ns, sys: 893 µs, total: 893 µs\n","Wall time: 863 µs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VoDEtjKAxywM","colab_type":"code","outputId":"c02c1c46-5cfd-4925-d9b8-683c384de4d7","executionInfo":{"status":"ok","timestamp":1565682399651,"user_tz":-120,"elapsed":392,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%time gpu_sqrt_kernel[128,128](d_a, d_out); cuda.synchronize()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 1.17 ms, sys: 516 µs, total: 1.68 ms\n","Wall time: 3.78 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JNhYXML25Uuj","colab_type":"code","colab":{}},"source":["\n","\n","from math import hypot\n","\n","@cuda.jit\n","def hypot_stride(a, b, out):\n","  idx = cuda.grid(1) \n","  out[idx] = hypot(a[idx], b[idx])\n","  \n","# You do not need to modify the contents in this cell\n","n = 1000000\n","a = np.random.uniform(-12, 12, n).astype(np.float32)\n","b = np.random.uniform(-12, 12, n).astype(np.float32)\n","d_a = cuda.to_device(a)\n","d_b = cuda.to_device(b)\n","d_c = cuda.device_array_like(d_b)\n","\n","blocks = 128\n","threads_per_block = 64\n","\n","hypot_stride[blocks, threads_per_block](d_a, d_b, d_c)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uw-imoG18q8d","colab_type":"code","outputId":"d0284a44-f588-49df-fed4-9a7baefbfa7e","executionInfo":{"status":"ok","timestamp":1565682644926,"user_tz":-120,"elapsed":376,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%time hypot_stride[1,1](d_a, d_b, d_c); cuda.synchronize()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 1.06 ms, sys: 20 µs, total: 1.08 ms\n","Wall time: 3.46 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jSHmjGO59ImI","colab_type":"code","outputId":"1805a033-608d-4dcc-a46f-0546770b1403","executionInfo":{"status":"ok","timestamp":1565682642885,"user_tz":-120,"elapsed":433,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%time hypot_stride[128,64](d_a, d_b, d_c); cuda.synchronize()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CPU times: user 852 µs, sys: 0 ns, total: 852 µs\n","Wall time: 966 µs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A4AKNG0bBb5M","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_atan_nostride(x, out):\n","  idx = cuda.grid(1)\n","  out[idx] = math.atan(x[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvD0WL9pAk_I","colab_type":"code","colab":{}},"source":["n = 10000000\n","a = np.random.uniform(-5, 5, n).astype(np.float32)\n","d_a = cuda.to_device(a)\n","d_out = cuda.device_array_like(d_a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWiY7nnBAuoK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"54c04712-36be-45ee-84ec-0233fe5d31d0","executionInfo":{"status":"ok","timestamp":1565683891278,"user_tz":-120,"elapsed":2812,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["%time gpu_atan_stride[1,1](d_a, d_out), cuda.synchronize()\n"],"execution_count":53,"outputs":[{"output_type":"stream","text":["CPU times: user 1.37 s, sys: 1.02 s, total: 2.39 s\n","Wall time: 2.4 s\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"NSr6bW5CA0CK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"286fb076-b0c9-46ab-b25b-d6a208faa001","executionInfo":{"status":"ok","timestamp":1565683902466,"user_tz":-120,"elapsed":390,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["%time gpu_atan_stride[128,64](d_a, d_out), cuda.synchronize()\n"],"execution_count":54,"outputs":[{"output_type":"stream","text":["CPU times: user 686 µs, sys: 1.85 ms, total: 2.53 ms\n","Wall time: 2 ms\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"Chmpm2EfBK9U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"546390e5-6eee-403d-960c-5f2e19073026","executionInfo":{"status":"ok","timestamp":1565684029474,"user_tz":-120,"elapsed":505,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["%time gpu_atan_nostride[1,1](d_a, d_out), cuda.synchronize()\n"],"execution_count":69,"outputs":[{"output_type":"stream","text":["CPU times: user 127 ms, sys: 1.55 ms, total: 129 ms\n","Wall time: 130 ms\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"z9JN8QWVBlQ5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"6f3b6927-56da-40c6-f27d-911818ed5302","executionInfo":{"status":"ok","timestamp":1565684001304,"user_tz":-120,"elapsed":384,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["%time gpu_atan_nostride[128,64](d_a, d_out), cuda.synchronize()\n"],"execution_count":64,"outputs":[{"output_type":"stream","text":["CPU times: user 1.65 ms, sys: 0 ns, total: 1.65 ms\n","Wall time: 3.05 ms\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"RyCdIpuOB27q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}