{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"numba_cuda_kernel.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fm8wdYqVBeXj","colab_type":"text"},"source":["## Numba + CUDA on Google Colab\n","\n","By default, Google Colab is not able to run numba + CUDA, because two lilbraries are not found, `libdevice` and `libnvvm.so`. So we need to make sure that these libraries are found in the notebook.  \n","\n","First, we look for these libraries on the system. To do that, we simply run the `find` command, to recursively look for these libraries starting at the root of the filesystem. The exclamation mark escapes the line so that it's executed by the Linux shell, and not by the jupyter notebook. "]},{"cell_type":"code","metadata":{"id":"5Mt7dgLwmnVJ","colab_type":"code","outputId":"88caa6c4-030f-430a-8ab1-c53a67c7b4b0","executionInfo":{"status":"ok","timestamp":1564487238428,"user_tz":-120,"elapsed":14553,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!find / -iname 'libdevice'\n","!find / -iname 'libnvvm.so'\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/cuda-10.0/nvvm/libdevice\n","/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HO0mdhPoDRTS","colab_type":"text"},"source":["Then, we add the two libraries to numba environment variables:"]},{"cell_type":"code","metadata":{"id":"Ctr6aM3cmkdx","colab_type":"code","colab":{}},"source":["import os\n","os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-10.0/nvvm/libdevice\"\n","os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"whscBGwEDcEZ","colab_type":"text"},"source":["And we're done! Now let's get started. "]},{"cell_type":"markdown","metadata":{"id":"iah5fiDRtVYt","colab_type":"text"},"source":["## A first CUDA kernel\n","\n","Let's get started by implementing a very simple CUDA kernel to compute the square root of each value in an array. First, here is our array: "]},{"cell_type":"code","metadata":{"id":"RGzqgz7DuK9n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f121b5b2-cc7d-424a-97b0-687a155d07a2","executionInfo":{"status":"ok","timestamp":1564488588319,"user_tz":-120,"elapsed":419,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["import numpy as np\n","a = np.arange(4096,dtype=np.float32)\n","a"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.000e+00, 1.000e+00, 2.000e+00, ..., 4.093e+03, 4.094e+03,\n","       4.095e+03], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"DroQCdZpuU5E","colab_type":"text"},"source":["As we have seen in [part II](https://thedatafrog.com/boost-python-gpu/), and as discussed in the introduction, we can simply use numba's vectorize decorator to compute the square root of all elements in parallel on the GPU: "]},{"cell_type":"code","metadata":{"id":"Dhmy-LmjuRCX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"3c3d2c72-e72d-40b4-f07c-aaffaa7a0524","executionInfo":{"status":"ok","timestamp":1564488591989,"user_tz":-120,"elapsed":408,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["import math\n","from numba import vectorize\n","\n","@vectorize(['float32(float32)'], target='cuda')\n","def gpu_sqrt(x):\n","    return math.sqrt(x)\n","  \n","gpu_sqrt(a)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n","       63.992188 ], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"i1LUuMmBu2sB","colab_type":"text"},"source":["This time, as an exercise, we'll do the same with a custom CUDA kernel. \n","\n","We first define our kernel: "]},{"cell_type":"code","metadata":{"id":"ln2DfOpjuvfp","colab_type":"code","colab":{}},"source":["from numba import cuda\n","\n","@cuda.jit\n","def gpu_sqrt_kernel(x, out):\n","  idx = cuda.grid(1)\n","  out[idx] = math.sqrt(x[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ruRzBxXgwZyh","colab_type":"text"},"source":["Let's discuss this code in some details. \n","\n","We have an input array of 4096 values, so we will use 4096 threads on the GPU. \n","\n","Our input and output arrays are one dimensional, so we will use a one-dimensional *grid* of threads (we will discuss grids in details in the next section). The call `cuda.grid(1)` returns the unique index for the current thread in the whole grid.  With 4096 threads, `idx` will range from 0 to 4095. \n","\n","Then, we see in the code that each thread is going to deal with a single element of the input array, producing a single element in the output array. \n","\n","Now, we copy our input array to the GPU device, create an output array on the device with the same shape, and finally launch the kernel: "]},{"cell_type":"code","metadata":{"id":"cN1flx7Dx_nT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0babbcf5-88da-49aa-d077-8099d27740a2","executionInfo":{"status":"ok","timestamp":1564489476012,"user_tz":-120,"elapsed":409,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["# move input data to the device\n","d_a = cuda.to_device(a)\n","# create output data on the device\n","d_out = cuda.device_array_like(d_a)\n","\n","# we decide to use 32 blocks, each containing 128 threads\n","blocks_per_grid = 32\n","threads_per_block = 128\n","gpu_sqrt_kernel[blocks_per_grid, threads_per_block](d_a, d_out)\n","# wait for all threads to complete\n","cuda.synchronize()\n","# copy the output array back to the host system\n","# and print it\n","print(d_out.copy_to_host())"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[ 0.         1.         1.4142135 ... 63.97656   63.98437   63.992188 ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TL3-d3S0z_RM","colab_type":"text"},"source":["**Exercises:**\n","\n","- Go back to the previous cell, and try to decrease the number of blocks per grid, or the number of threads per block. \n","- Then try to increase the number of blocks per grid, or the number of threads per blocks\n","- Try to remove the `cuda.synchronize()` call\n","\n","**Results**: \n","\n","- When you reduce the number of threads, either by decreasing the number of blocks per grid or the number of threads per block, some elements are not processed, and the corresponding slots at the end of the output array remain set to their default value, which is 0. \n","- If, on the other hand, you increase the number of threads, it seems that everything is working fine. However, this actually creates an error even though we cannot see it. We will see later how to expose this error. Debugging code is one of the difficulties of CUDA, as the error messages are not always visible.\n","- Finally, you might have expected that commenting out the call to `cuda.synchronize` would have resulted in output array only partially filled, or not filled at all. That would be a good guess since the CPU keeps running the main program (the one of the cell) while the GPU processes the data asynchronously. However, the copy to the host performs an implicit synchronization, so the call to cuda.syncronize is not necessary.  "]},{"cell_type":"markdown","metadata":{"id":"Ces_Ss67GqzR","colab_type":"text"},"source":["## Execution configuration : grid, blocks, and threads\n","\n","In our first example above, we decided to use 4096 threads. These threads were arranged as a one-dimensional grid containing 32 blocks of 128 threads each. That's quite mysterious and in this section, I will try and answer the questions you may have. \n","\n","First, what is a grid? We have used a one dimensional grid in our simple example but can a grid have more dimensions? Why would we need that? \n","\n","Then, why do we need to arrange threads into blocks within the grid? This is actually driven by the hardware, so we will need to learn a bit more about the GPU architecture to understand that better. \n","\n","Finally, how did we come up with the magical numbers 32 and 128? Could we use other values like 16 and 256 (which still total 4096), or 64 and 64? How to make this choice? \n","\n"]},{"cell_type":"code","metadata":{"id":"VxhecsBfz-cT","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAH9lfmuIIRF","colab_type":"text"},"source":["Let's find out which GPU we are using on the Google Colab platform:"]},{"cell_type":"code","metadata":{"id":"fACSmHLzJanZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"ebe78383-af6d-4e98-f5ff-97450ae39767","executionInfo":{"status":"ok","timestamp":1564495194577,"user_tz":-120,"elapsed":405,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["cuda.detect()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Found 1 CUDA devices\n","id 0             b'Tesla T4'                              [SUPPORTED]\n","                      compute capability: 7.5\n","                           pci device id: 4\n","                              pci bus id: 0\n","Summary:\n","\t1/1 devices are supported\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"BM_fu2ZvM5dJ","colab_type":"text"},"source":["We see that's an nvidia [T4](https://www.nvidia.com/en-us/data-center/tesla-t4/), and here is the [white paper with the specs](https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf) for the Turing architecture. In this paper, we see that the T4 is based on the Turing TU102 GPU which has: \n","\n","- 72 streaming multiprocessors\n","- 64 cuda cores per multiprocessor\n","\n","Here is a schematic view of the TU102 GPU: \n","\n","![TU102 GPU](tu_102_diagram.jpg)"]},{"cell_type":"code","metadata":{"id":"YM2Q8ys3LKaI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}